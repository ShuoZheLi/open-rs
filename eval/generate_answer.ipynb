{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c8293bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-02 19:31:39 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 19:31:40,076\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from vllm import LLM, SamplingParams\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "from latex2sympy2_extended import NormalizationConfig\n",
    "from math_verify import LatexExtractionConfig, parse\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7cc781a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-02 19:31:49 config.py:542] This model supports multiple tasks: {'embed', 'score', 'classify', 'reward', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 08-02 19:31:49 config.py:1401] Defaulting to use mp for distributed inference\n",
      "WARNING 08-02 19:31:49 arg_utils.py:1135] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 08-02 19:31:49 config.py:1556] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 08-02 19:31:49 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='/nfs/shuozhe/saved_model/Llama-3.2-1B', speculative_config=None, tokenizer='/nfs/shuozhe/saved_model/Llama-3.2-1B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/nfs/shuozhe/saved_model/Llama-3.2-1B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 08-02 19:31:50 multiproc_worker_utils.py:300] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 08-02 19:31:50 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=196709)\u001b[0;0m INFO 08-02 19:31:50 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=196712)\u001b[0;0m INFO 08-02 19:31:50 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=196717)\u001b[0;0m INFO 08-02 19:31:50 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "INFO 08-02 19:31:51 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=196709)\u001b[0;0m INFO 08-02 19:31:51 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=196712)\u001b[0;0m INFO 08-02 19:31:51 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=196717)\u001b[0;0m INFO 08-02 19:31:51 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 08-02 19:31:52 utils.py:950] Found nccl from library libnccl.so.2\n",
      "INFO 08-02 19:31:52 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=196709)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=196712)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=196717)\u001b[0;0m INFO 08-02 19:31:52 utils.py:950] Found nccl from library libnccl.so.2\n",
      "INFO 08-02 19:31:52 utils.py:950] Found nccl from library libnccl.so.2\n",
      "INFO 08-02 19:31:52 utils.py:950] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=196709)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=196712)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=196717)\u001b[0;0m INFO 08-02 19:31:52 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 08-02 19:31:52 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 08-02 19:31:52 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "WARNING 08-02 19:31:53 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=196709)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=196712)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=196717)\u001b[0;0m WARNING 08-02 19:31:53 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 08-02 19:31:53 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 08-02 19:31:53 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 08-02 19:31:53 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_a0affd0f'), local_subscribe_port=51391, remote_subscribe_port=None)\n",
      "INFO 08-02 19:31:53 model_runner.py:1110] Starting to load model /nfs/shuozhe/saved_model/Llama-3.2-1B...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=196709)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=196717)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=196712)\u001b[0;0m INFO 08-02 19:31:53 model_runner.py:1110] Starting to load model /nfs/shuozhe/saved_model/Llama-3.2-1B...\n",
      "INFO 08-02 19:31:53 model_runner.py:1110] Starting to load model /nfs/shuozhe/saved_model/Llama-3.2-1B...\n",
      "INFO 08-02 19:31:53 model_runner.py:1110] Starting to load model /nfs/shuozhe/saved_model/Llama-3.2-1B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.91it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.90it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-02 19:31:54 model_runner.py:1115] Loading model weights took 0.5968 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=196717)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=196712)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=196709)\u001b[0;0m INFO 08-02 19:31:54 model_runner.py:1115] Loading model weights took 0.5968 GB\n",
      "INFO 08-02 19:31:54 model_runner.py:1115] Loading model weights took 0.5968 GB\n",
      "INFO 08-02 19:31:54 model_runner.py:1115] Loading model weights took 0.5968 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=196717)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=196712)\u001b[0;0m INFO 08-02 19:31:59 worker.py:267] Memory profiling takes 5.52 seconds\n",
      "INFO 08-02 19:31:59 worker.py:267] Memory profiling takes 5.52 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=196712)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=196717)\u001b[0;0m INFO 08-02 19:31:59 worker.py:267] the current vLLM instance can use total_gpu_memory (79.15GiB) x gpu_memory_utilization (0.70) = 55.41GiB\n",
      "INFO 08-02 19:31:59 worker.py:267] the current vLLM instance can use total_gpu_memory (79.15GiB) x gpu_memory_utilization (0.70) = 55.41GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=196712)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=196717)\u001b[0;0m INFO 08-02 19:31:59 worker.py:267] model weights take 0.60GiB; non_torch_memory takes 0.35GiB; PyTorch activation peak memory takes 0.05GiB; the rest of the memory reserved for KV Cache is 54.40GiB.\n",
      "INFO 08-02 19:31:59 worker.py:267] model weights take 0.60GiB; non_torch_memory takes 0.35GiB; PyTorch activation peak memory takes 0.05GiB; the rest of the memory reserved for KV Cache is 54.40GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=196709)\u001b[0;0m INFO 08-02 19:31:59 worker.py:267] Memory profiling takes 5.53 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=196709)\u001b[0;0m INFO 08-02 19:31:59 worker.py:267] the current vLLM instance can use total_gpu_memory (79.15GiB) x gpu_memory_utilization (0.70) = 55.41GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=196709)\u001b[0;0m INFO 08-02 19:31:59 worker.py:267] model weights take 0.60GiB; non_torch_memory takes 0.36GiB; PyTorch activation peak memory takes 0.05GiB; the rest of the memory reserved for KV Cache is 54.39GiB.\n",
      "INFO 08-02 19:31:59 worker.py:267] Memory profiling takes 5.61 seconds\n",
      "INFO 08-02 19:31:59 worker.py:267] the current vLLM instance can use total_gpu_memory (79.15GiB) x gpu_memory_utilization (0.70) = 55.41GiB\n",
      "INFO 08-02 19:31:59 worker.py:267] model weights take 0.60GiB; non_torch_memory takes 0.36GiB; PyTorch activation peak memory takes 1.18GiB; the rest of the memory reserved for KV Cache is 53.27GiB.\n",
      "INFO 08-02 19:32:00 executor_base.py:110] # CUDA blocks: 436366, # CPU blocks: 32768\n",
      "INFO 08-02 19:32:00 executor_base.py:115] Maximum concurrency for 131072 tokens per request: 53.27x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=196709)\u001b[0;0m INFO 08-02 19:32:02 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-02 19:32:02 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=196712)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=196717)\u001b[0;0m INFO 08-02 19:32:02 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 08-02 19:32:02 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:16<00:00,  2.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=196712)\u001b[0;0m INFO 08-02 19:32:19 model_runner.py:1562] Graph capturing finished in 16 secs, took 0.63 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=196717)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=196709)\u001b[0;0m INFO 08-02 19:32:19 model_runner.py:1562] Graph capturing finished in 16 secs, took 0.62 GiB\n",
      "INFO 08-02 19:32:19 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 25.10 seconds\n",
      "INFO 08-02 19:32:19 model_runner.py:1562] Graph capturing finished in 16 secs, took 0.62 GiB\n",
      "INFO 08-02 19:32:19 model_runner.py:1562] Graph capturing finished in 17 secs, took 0.62 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"/home/edwardhu/workspace/shuozhe/open-rs/data/Qwen2-1.5B_sfted_noKL_math_7500/checkpoint-1875\"\n",
    "# model_name = \"/home/edwardhu/workspace/shuozhe/open-rs/data/Qwen2-1.5B_sfted_4e2_skl_math_7500/checkpoint-1875\"\n",
    "# model_name = \"/home/edwardhu/workspace/shuozhe/open-rs/data/Qwen2.5-1.5B-sft_entropy_3/checkpoint-42\"\n",
    "model_name = \"/nfs/shuozhe/saved_model/Llama-3.2-1B\"\n",
    "\n",
    "\n",
    "num_gpus = 4\n",
    "llm = None\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Create LLM object\n",
    "llm = LLM(model=model_name,  # replace your own model\n",
    "            dtype='bfloat16',\n",
    "            tensor_parallel_size=num_gpus,  # number of gpu\n",
    "            gpu_memory_utilization=0.7,  # prevent OOM\n",
    "            trust_remote_code=True,\n",
    "            # use_cache=False,\n",
    "            )\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer, and put your final answer within \\\\boxed{{}} . The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. Note that respond by English, NOT use other languages.\"\n",
    ")\n",
    "\n",
    "# SYSTEM_PROMPT = (\n",
    "#     \"Summarize the following conversation.\"\n",
    "# )\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.7,\n",
    "                                     max_tokens=1024,\n",
    "                                     )\n",
    "\n",
    "def process_single_prompt(question, tokenizer):\n",
    "    prompt = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\",   \"content\": question},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(\n",
    "        prompt,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3abf4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 2337/2337 [03:08<00:00, 12.40it/s, est. speed input: 1643.86 toks/s, output: 6739.98 toks/s]\n"
     ]
    }
   ],
   "source": [
    "# load huggingface dataset\n",
    "dataset = load_dataset(\"/nfs/shuozhe/saved_dataset/selfAware\", split=\"train\")\n",
    "# only keep the rows with \"answerable\" field to be False\n",
    "dataset = dataset.filter(lambda x: x[\"answerable\"] == True)\n",
    "\n",
    "# go through all the \"question\" field in the dataset and generate the answer\n",
    "questions = dataset[\"question\"]\n",
    "gt_answers = dataset[\"answer\"]\n",
    "\n",
    "# Prepare batched prompts\n",
    "prompts = [process_single_prompt(q, tokenizer) for q in questions]\n",
    "\n",
    "# Generate all responses at once (in batch)\n",
    "responses = llm.generate(prompts, sampling_params=sampling_params)\n",
    "\n",
    "# Extract answers\n",
    "answers = [res.outputs[0].text.strip() for res in responses]\n",
    "\n",
    "# Save the results to a JSON file\n",
    "output_file = \"Qwen2.5-1.5B-sft_entropy_3_42.json\"\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump({\"questions\": questions, \"answers\": answers, \"gt_answers\": gt_answers}, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ee829b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2337/2337 [16:55<00:00,  2.30it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 22.89%\n"
     ]
    }
   ],
   "source": [
    "# Load the model outputs\n",
    "output_file = \"Qwen2.5-1.5B-sft_entropy_3_42.json\"\n",
    "with open(output_file, 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "questions = data[\"questions\"]\n",
    "answers = data[\"answers\"]\n",
    "gt_answers = data[\"gt_answers\"]  # list of lists\n",
    "\n",
    "assert len(questions) == len(answers) == len(gt_answers)\n",
    "\n",
    "# Extract boxed answer from model output\n",
    "def extract_boxed_answer(answer):\n",
    "    match = re.search(r'\\\\boxed{(.*?)}', answer)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "    return answer.strip()\n",
    "\n",
    "\n",
    "# This is the new-style client instantiation\n",
    "# client = openai.OpenAI(api_key=os.environ[\"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\"])\n",
    "\n",
    "client = openai.OpenAI(api_key=\"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\")\n",
    "\n",
    "\n",
    "def verify_with_chatgpt(pred, gt_list, question):\n",
    "    system_prompt = (\n",
    "        \"You are a grader for a math exam. \"\n",
    "        \"Given a math question, a predicted answer, and a list of acceptable answers, \"\n",
    "        \"determine whether the predicted answer is semantically or numerically equivalent \"\n",
    "        \"to any of the acceptable answers. Respond with only 'Yes' or 'No'.\"\n",
    "    )\n",
    "    user_prompt = (\n",
    "        f\"Question: {question}\\n\"\n",
    "        f\"Predicted Answer: {pred}\\n\"\n",
    "        f\"Ground Truth Answers: {gt_list}\\n\"\n",
    "        f\"Is the predicted answer correct?\"\n",
    "    )\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    \n",
    "    result = response.choices[0].message.content.strip().lower()\n",
    "    return result.startswith(\"yes\")\n",
    "\n",
    "# Run evaluation\n",
    "correct = 0\n",
    "total = len(questions)\n",
    "results = []\n",
    "\n",
    "for q, pred, gt in tqdm(zip(questions, answers, gt_answers), total=total):\n",
    "    pred_clean = extract_boxed_answer(pred)\n",
    "    is_correct = verify_with_chatgpt(pred_clean, gt, q)\n",
    "    correct += int(is_correct)\n",
    "    # if is_correct:\n",
    "    #     print(f\"Correct: {q} -> {pred_clean}\")\n",
    "    # else:\n",
    "    #     print(f\"Incorrect: {q} -> {pred_clean} (Ground Truth: {gt})\")\n",
    "    results.append({\"question\": q, \"prediction\": pred_clean, \"gt\": gt, \"correct\": is_correct})\n",
    "\n",
    "# Accuracy report\n",
    "accuracy = correct / total\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# Optional: save detailed results\n",
    "with open(\"Qwen2.5-1.5B-sft_entropy_3_42_grading_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xr1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
