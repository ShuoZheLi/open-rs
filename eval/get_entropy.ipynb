{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ac5ffa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-07 21:09:49 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-07 21:09:49,409\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from vllm import LLM, SamplingParams\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21ebf4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-07 21:09:58 config.py:542] This model supports multiple tasks: {'classify', 'reward', 'embed', 'score', 'generate'}. Defaulting to 'generate'.\n",
      "INFO 07-07 21:09:58 config.py:1401] Defaulting to use mp for distributed inference\n",
      "WARNING 07-07 21:09:58 arg_utils.py:1135] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 07-07 21:09:58 config.py:1556] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 07-07 21:09:58 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='/home/edwardhu/workspace/shuozhe/open-rs/data/Qwen2.5-1.5B_noKL_new/checkpoint-500', speculative_config=None, tokenizer='/home/edwardhu/workspace/shuozhe/open-rs/data/Qwen2.5-1.5B_noKL_new/checkpoint-500', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/edwardhu/workspace/shuozhe/open-rs/data/Qwen2.5-1.5B_noKL_new/checkpoint-500, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 07-07 21:09:59 multiproc_worker_utils.py:300] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 07-07 21:09:59 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3203780)\u001b[0;0m INFO 07-07 21:09:59 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3203783)\u001b[0;0m INFO 07-07 21:09:59 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3203788)\u001b[0;0m INFO 07-07 21:09:59 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "INFO 07-07 21:10:00 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3203780)\u001b[0;0m INFO 07-07 21:10:01 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3203783)\u001b[0;0m INFO 07-07 21:10:01 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3203788)\u001b[0;0m INFO 07-07 21:10:01 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 07-07 21:10:01 utils.py:950] Found nccl from library libnccl.so.2\n",
      "INFO 07-07 21:10:02 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3203780)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3203788)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3203783)\u001b[0;0m INFO 07-07 21:10:01 utils.py:950] Found nccl from library libnccl.so.2\n",
      "INFO 07-07 21:10:01 utils.py:950] Found nccl from library libnccl.so.2\n",
      "INFO 07-07 21:10:01 utils.py:950] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3203780)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3203783)\u001b[0;0m INFO 07-07 21:10:02 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3203788)\u001b[0;0m INFO 07-07 21:10:02 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 07-07 21:10:02 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "WARNING 07-07 21:10:02 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3203788)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3203783)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3203780)\u001b[0;0m WARNING 07-07 21:10:02 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 07-07 21:10:02 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 07-07 21:10:02 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 07-07 21:10:02 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_99b20bab'), local_subscribe_port=50889, remote_subscribe_port=None)\n",
      "INFO 07-07 21:10:02 model_runner.py:1110] Starting to load model /home/edwardhu/workspace/shuozhe/open-rs/data/Qwen2.5-1.5B_noKL_new/checkpoint-500...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3203783)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3203788)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3203780)\u001b[0;0m INFO 07-07 21:10:02 model_runner.py:1110] Starting to load model /home/edwardhu/workspace/shuozhe/open-rs/data/Qwen2.5-1.5B_noKL_new/checkpoint-500...\n",
      "INFO 07-07 21:10:02 model_runner.py:1110] Starting to load model /home/edwardhu/workspace/shuozhe/open-rs/data/Qwen2.5-1.5B_noKL_new/checkpoint-500...\n",
      "INFO 07-07 21:10:02 model_runner.py:1110] Starting to load model /home/edwardhu/workspace/shuozhe/open-rs/data/Qwen2.5-1.5B_noKL_new/checkpoint-500...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.39it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.37it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=3203788)\u001b[0;0m INFO 07-07 21:10:03 model_runner.py:1115] Loading model weights took 0.7894 GB\n",
      "INFO 07-07 21:10:03 model_runner.py:1115] Loading model weights took 0.7894 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3203780)\u001b[0;0m INFO 07-07 21:10:03 model_runner.py:1115] Loading model weights took 0.7894 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3203783)\u001b[0;0m INFO 07-07 21:10:03 model_runner.py:1115] Loading model weights took 0.7894 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3203788)\u001b[0;0m INFO 07-07 21:10:09 worker.py:267] Memory profiling takes 5.79 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3203788)\u001b[0;0m INFO 07-07 21:10:09 worker.py:267] the current vLLM instance can use total_gpu_memory (79.15GiB) x gpu_memory_utilization (0.70) = 55.41GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3203788)\u001b[0;0m INFO 07-07 21:10:09 worker.py:267] model weights take 0.79GiB; non_torch_memory takes 0.35GiB; PyTorch activation peak memory takes 0.06GiB; the rest of the memory reserved for KV Cache is 54.20GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3203783)\u001b[0;0m INFO 07-07 21:10:09 worker.py:267] Memory profiling takes 5.81 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3203783)\u001b[0;0m INFO 07-07 21:10:09 worker.py:267] the current vLLM instance can use total_gpu_memory (79.15GiB) x gpu_memory_utilization (0.70) = 55.41GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3203783)\u001b[0;0m INFO 07-07 21:10:09 worker.py:267] model weights take 0.79GiB; non_torch_memory takes 0.35GiB; PyTorch activation peak memory takes 0.06GiB; the rest of the memory reserved for KV Cache is 54.20GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3203780)\u001b[0;0m INFO 07-07 21:10:09 worker.py:267] Memory profiling takes 5.82 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3203780)\u001b[0;0m INFO 07-07 21:10:09 worker.py:267] the current vLLM instance can use total_gpu_memory (79.15GiB) x gpu_memory_utilization (0.70) = 55.41GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3203780)\u001b[0;0m INFO 07-07 21:10:09 worker.py:267] model weights take 0.79GiB; non_torch_memory takes 0.36GiB; PyTorch activation peak memory takes 0.06GiB; the rest of the memory reserved for KV Cache is 54.19GiB.\n",
      "INFO 07-07 21:10:09 worker.py:267] Memory profiling takes 5.89 seconds\n",
      "INFO 07-07 21:10:09 worker.py:267] the current vLLM instance can use total_gpu_memory (79.15GiB) x gpu_memory_utilization (0.70) = 55.41GiB\n",
      "INFO 07-07 21:10:09 worker.py:267] model weights take 0.79GiB; non_torch_memory takes 0.36GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 52.86GiB.\n",
      "INFO 07-07 21:10:09 executor_base.py:110] # CUDA blocks: 247459, # CPU blocks: 18724\n",
      "INFO 07-07 21:10:09 executor_base.py:115] Maximum concurrency for 131072 tokens per request: 30.21x\n",
      "INFO 07-07 21:10:13 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3203788)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3203780)\u001b[0;0m INFO 07-07 21:10:13 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 07-07 21:10:13 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3203783)\u001b[0;0m INFO 07-07 21:10:13 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:17<00:00,  2.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=3203780)\u001b[0;0m INFO 07-07 21:10:31 model_runner.py:1562] Graph capturing finished in 17 secs, took 1.06 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3203788)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3203783)\u001b[0;0m INFO 07-07 21:10:31 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 27.66 seconds\n",
      "INFO 07-07 21:10:31 model_runner.py:1562] Graph capturing finished in 17 secs, took 1.06 GiB\n",
      "INFO 07-07 21:10:31 model_runner.py:1562] Graph capturing finished in 17 secs, took 1.06 GiB\n",
      "INFO 07-07 21:10:31 model_runner.py:1562] Graph capturing finished in 17 secs, took 1.06 GiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"/home/edwardhu/workspace/shuozhe/open-rs/data/Qwen2.5-1.5B_noKL_new/checkpoint-500\"\n",
    "# MODEL_NAME = \"/nfs/shuozhe/saved_model/Qwen2.5-1.5B\"\n",
    "NUM_GPUS = 4\n",
    "SYSTEM_PROMPT = \"\"\"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer, and put your final answer within \\\\boxed{{}} . The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. Note that respond by English, NOT use other languages.\"\"\"\n",
    "\n",
    "# ------------ 1.  shared tokenizer ---------------\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# ------------ 2.  inference engine (vLLM) --------\n",
    "llm = LLM(\n",
    "    model=MODEL_NAME,\n",
    "    dtype=\"bfloat16\",\n",
    "    tensor_parallel_size=NUM_GPUS,\n",
    "    gpu_memory_utilization=0.7,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "sampler = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    max_tokens=1024,\n",
    "    # any other decoding knobs you need\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e9158c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ 1. Prompt builder -------------------\n",
    "def build_chat_prompt(user_msg: str) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\",   \"content\": user_msg},\n",
    "    ]\n",
    "    return tok.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "# ------------ 2. Entropy Helper -------------------\n",
    "class EntropyMeter:\n",
    "    def __init__(self, model_name: str, dtype=torch.bfloat16):\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=dtype,\n",
    "            device_map=\"auto\",      # Adjust if needed: {\"\": \"cpu\"} for safety\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        self.pad_id = tok.pad_token_id or tok.eos_token_id\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def entropy_for_batch(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        completions: List[str],\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Return per-token entropy (B, C) and mean entropy (B,)\"\"\"\n",
    "        full_texts = [p + c for p, c in zip(prompts, completions)]\n",
    "        enc = tok(full_texts, return_tensors=\"pt\", padding=True).to(self.model.device)\n",
    "        input_ids, attn_mask = enc[\"input_ids\"], enc[\"attention_mask\"]\n",
    "\n",
    "        # forward\n",
    "        logits = self.model(input_ids=input_ids, attention_mask=attn_mask).logits\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        log_probs = torch.log_softmax(logits, dim=-1)\n",
    "        token_entropy = -(probs * log_probs).sum(-1)      # (B, seq_len)\n",
    "\n",
    "        # ── isolate only the *completion* tokens ──────────────────────────────\n",
    "        prompt_lens = [len(tok(p)[\"input_ids\"]) for p in prompts]\n",
    "        mask = torch.zeros_like(token_entropy, dtype=torch.bool)\n",
    "        for row, plen in enumerate(prompt_lens):\n",
    "            mask[row, plen:] = True                # True → completion positions\n",
    "\n",
    "        # keep only completion-token entropies\n",
    "        token_entropy = token_entropy.masked_fill(~mask, float(\"nan\"))  # or 0.0\n",
    "\n",
    "        mean_entropy = (\n",
    "            token_entropy.nan_to_num(0.0).sum(-1) / mask.sum(-1).clamp(min=1)\n",
    "        )   # (B,)\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"token_entropy\": token_entropy,\n",
    "            \"mean_entropy\":  mean_entropy,\n",
    "        }\n",
    "\n",
    "# ------------ 3. Main Generation + Entropy -------------------\n",
    "def generate_with_entropy_one_by_one(user_prompts: List[str]):\n",
    "    # 3-a build chat-formatted prompts\n",
    "    chat_prompts = [build_chat_prompt(u) for u in user_prompts]\n",
    "\n",
    "    # 3-b generate completions (batched via vLLM)\n",
    "    outs = llm.generate(chat_prompts, sampling_params=sampler)\n",
    "    completions = [o.outputs[0].text for o in outs]\n",
    "\n",
    "    # 3-c compute entropy one-by-one to avoid GPU OOM\n",
    "    meter = EntropyMeter(MODEL_NAME)\n",
    "    token_entropies, mean_entropies = [], []\n",
    "\n",
    "    for prompt, completion in zip(chat_prompts, completions):\n",
    "        ent = meter.entropy_for_batch([prompt], [completion])\n",
    "        token_entropies.append(ent[\"token_entropy\"][0])   # strip batch dim\n",
    "        mean_entropies.append(ent[\"mean_entropy\"][0])\n",
    "        del ent\n",
    "        # torch.cuda.empty_cache()  # optional but helps avoid buildup\n",
    "\n",
    "    return {\n",
    "        \"token_entropy\": token_entropies,                # List[Tensor(seq_len)]\n",
    "        \"mean_entropy\": torch.stack(mean_entropies),     # Tensor(B,)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee09905",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 100/100 [00:10<00:00,  9.58it/s, est. speed input: 1295.64 toks/s, output: 4207.84 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8438, device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# batch = [\n",
    "#         \"Where are all the aliens located?\",\n",
    "#         \"What is the full name of the person who invented invisible unicorns?\",\n",
    "#     ]\n",
    "\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Load the dataset\n",
    "with open(\"SelfAware.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract the list of questions\n",
    "examples = data[\"example\"]\n",
    "\n",
    "# Filter for unanswerable questions\n",
    "unanswerable_questions = [item[\"question\"] for item in examples if not item.get(\"answerable\", True)]\n",
    "\n",
    "# set random seed for reproducibility\n",
    "random.seed(70)\n",
    "\n",
    "# Randomly select 10\n",
    "sample_questions = random.sample(unanswerable_questions, 100)\n",
    "\n",
    "# Format them for use\n",
    "batch = sample_questions\n",
    "\n",
    "result = generate_with_entropy_one_by_one(batch)\n",
    "print(result[\"mean_entropy\"].mean())  # Print the mean entropy for each prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a258c8b",
   "metadata": {},
   "source": [
    "<!-- DS-qwen1.5b tensor(1.2578, device='cuda:0', dtype=torch.bfloat16) -->\n",
    "<!-- tensor(1.2578, device='cuda:0', dtype=torch.bfloat16) -->\n",
    "<!-- tensor(1.2500, device='cuda:0', dtype=torch.bfloat16) -->\n",
    "<!-- tensor(1.2656, device='cuda:0', dtype=torch.bfloat16) -->\n",
    "<!-- tensor(1.2656, device='cuda:0', dtype=torch.bfloat16) -->\n",
    "\n",
    "\n",
    "\n",
    "<!-- tensor(1.2109, device='cuda:0', dtype=torch.bfloat16) -->\n",
    "<!-- tensor(1.2500, device='cuda:0', dtype=torch.bfloat16) -->\n",
    "<!-- tensor(1.2266, device='cuda:0', dtype=torch.bfloat16) -->\n",
    "<!-- tensor(1.2500, device='cuda:0', dtype=torch.bfloat16) -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f33564f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 10/10 [00:04<00:00,  2.44it/s, est. speed input: 294.27 toks/s, output: 1126.15 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6250, dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "# batch = [\n",
    "#         \"Where are all the aliens located?\",\n",
    "#         \"What is the full name of the person who invented invisible unicorns?\",\n",
    "#     ]\n",
    "\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Load the dataset\n",
    "with open(\"SelfAware.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract the list of questions\n",
    "examples = data[\"example\"]\n",
    "\n",
    "# Filter for unanswerable questions\n",
    "unanswerable_questions = [item[\"question\"] for item in examples if not item.get(\"answerable\", True)]\n",
    "\n",
    "# set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Randomly select 10\n",
    "sample_questions = random.sample(unanswerable_questions, 10)\n",
    "\n",
    "# Format them for use\n",
    "batch = sample_questions\n",
    "\n",
    "result = generate_with_entropy(batch)\n",
    "print(result[\"mean_entropy\"].mean())  # Print the mean entropy for each prompt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xr1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
