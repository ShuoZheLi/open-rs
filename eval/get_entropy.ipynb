{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ac5ffa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-26 15:08:39 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 15:08:39,809\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from vllm import LLM, SamplingParams\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21ebf4bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-26 15:08:47 config.py:542] This model supports multiple tasks: {'classify', 'generate', 'reward', 'score', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 06-26 15:08:48 config.py:1401] Defaulting to use mp for distributed inference\n",
      "WARNING 06-26 15:08:48 arg_utils.py:1135] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 06-26 15:08:48 config.py:1556] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 06-26 15:08:48 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='/home/edwardhu/workspace/shuozhe/open-rs/data/R1-Distill-Qwen-1.5B-noKL-1500/checkpoint-100', speculative_config=None, tokenizer='/home/edwardhu/workspace/shuozhe/open-rs/data/R1-Distill-Qwen-1.5B-noKL-1500/checkpoint-100', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/edwardhu/workspace/shuozhe/open-rs/data/R1-Distill-Qwen-1.5B-noKL-1500/checkpoint-100, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 06-26 15:08:48 multiproc_worker_utils.py:300] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 06-26 15:08:48 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3158348)\u001b[0;0m INFO 06-26 15:08:48 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3158351)\u001b[0;0m INFO 06-26 15:08:48 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3158356)\u001b[0;0m INFO 06-26 15:08:48 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "INFO 06-26 15:08:49 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3158356)\u001b[0;0m INFO 06-26 15:08:49 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3158351)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3158348)\u001b[0;0m INFO 06-26 15:08:49 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 06-26 15:08:49 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 06-26 15:08:50 utils.py:950] Found nccl from library libnccl.so.2\n",
      "INFO 06-26 15:08:50 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3158348)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3158356)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3158351)\u001b[0;0m INFO 06-26 15:08:50 utils.py:950] Found nccl from library libnccl.so.2\n",
      "INFO 06-26 15:08:50 utils.py:950] Found nccl from library libnccl.so.2\n",
      "INFO 06-26 15:08:50 utils.py:950] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3158348)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3158356)\u001b[0;0m INFO 06-26 15:08:50 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3158351)\u001b[0;0m INFO 06-26 15:08:50 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 06-26 15:08:50 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "WARNING 06-26 15:08:51 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3158356)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3158348)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3158351)\u001b[0;0m WARNING 06-26 15:08:51 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 06-26 15:08:51 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 06-26 15:08:51 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 06-26 15:08:51 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_4dab6467'), local_subscribe_port=55985, remote_subscribe_port=None)\n",
      "INFO 06-26 15:08:51 model_runner.py:1110] Starting to load model /home/edwardhu/workspace/shuozhe/open-rs/data/R1-Distill-Qwen-1.5B-noKL-1500/checkpoint-100...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3158348)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3158356)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3158351)\u001b[0;0m INFO 06-26 15:08:51 model_runner.py:1110] Starting to load model /home/edwardhu/workspace/shuozhe/open-rs/data/R1-Distill-Qwen-1.5B-noKL-1500/checkpoint-100...\n",
      "INFO 06-26 15:08:51 model_runner.py:1110] Starting to load model /home/edwardhu/workspace/shuozhe/open-rs/data/R1-Distill-Qwen-1.5B-noKL-1500/checkpoint-100...\n",
      "INFO 06-26 15:08:51 model_runner.py:1110] Starting to load model /home/edwardhu/workspace/shuozhe/open-rs/data/R1-Distill-Qwen-1.5B-noKL-1500/checkpoint-100...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.54it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  3.52it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=3158348)\u001b[0;0m INFO 06-26 15:08:52 model_runner.py:1115] Loading model weights took 0.8988 GB\n",
      "INFO 06-26 15:08:52 model_runner.py:1115] Loading model weights took 0.8988 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3158356)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3158351)\u001b[0;0m INFO 06-26 15:08:52 model_runner.py:1115] Loading model weights took 0.8988 GB\n",
      "INFO 06-26 15:08:52 model_runner.py:1115] Loading model weights took 0.8988 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3158351)\u001b[0;0m INFO 06-26 15:08:55 worker.py:267] Memory profiling takes 2.81 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3158351)\u001b[0;0m INFO 06-26 15:08:55 worker.py:267] the current vLLM instance can use total_gpu_memory (79.15GiB) x gpu_memory_utilization (0.50) = 39.58GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3158351)\u001b[0;0m INFO 06-26 15:08:55 worker.py:267] model weights take 0.90GiB; non_torch_memory takes 0.35GiB; PyTorch activation peak memory takes 0.06GiB; the rest of the memory reserved for KV Cache is 38.26GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3158356)\u001b[0;0m INFO 06-26 15:08:55 worker.py:267] Memory profiling takes 2.83 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3158356)\u001b[0;0m INFO 06-26 15:08:55 worker.py:267] the current vLLM instance can use total_gpu_memory (79.15GiB) x gpu_memory_utilization (0.50) = 39.58GiB\n",
      "INFO 06-26 15:08:55 worker.py:267] Memory profiling takes 2.91 seconds\n",
      "INFO 06-26 15:08:55 worker.py:267] the current vLLM instance can use total_gpu_memory (79.15GiB) x gpu_memory_utilization (0.50) = 39.58GiB\n",
      "INFO 06-26 15:08:55 worker.py:267] model weights take 0.90GiB; non_torch_memory takes 0.36GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 36.92GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3158356)\u001b[0;0m INFO 06-26 15:08:55 executor_base.py:110] # CUDA blocks: 172844, # CPU blocks: 18724\n",
      "INFO 06-26 15:08:55 worker.py:267] model weights take 0.90GiB; non_torch_memory takes 0.35GiB; PyTorch activation peak memory takes 0.06GiB; the rest of the memory reserved for KV Cache is 38.26GiB.\n",
      "INFO 06-26 15:08:55 executor_base.py:115] Maximum concurrency for 131072 tokens per request: 21.10x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3158348)\u001b[0;0m INFO 06-26 15:08:55 worker.py:267] Memory profiling takes 2.76 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3158348)\u001b[0;0m INFO 06-26 15:08:55 worker.py:267] the current vLLM instance can use total_gpu_memory (79.15GiB) x gpu_memory_utilization (0.50) = 39.58GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3158348)\u001b[0;0m INFO 06-26 15:08:55 worker.py:267] model weights take 0.90GiB; non_torch_memory takes 0.36GiB; PyTorch activation peak memory takes 0.06GiB; the rest of the memory reserved for KV Cache is 38.25GiB.\n",
      "INFO 06-26 15:08:59 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3158356)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3158348)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3158351)\u001b[0;0m INFO 06-26 15:08:59 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-26 15:08:59 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-26 15:08:59 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:  97%|█████████▋| 34/35 [00:15<00:00,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=3158348)\u001b[0;0m "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:16<00:00,  2.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-26 15:09:16 model_runner.py:1562] Graph capturing finished in 17 secs, took 1.06 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3158356)\u001b[0;0m INFO 06-26 15:09:16 model_runner.py:1562] Graph capturing finished in 17 secs, took 1.06 GiB\n",
      "INFO 06-26 15:09:16 model_runner.py:1562] Graph capturing finished in 17 secs, took 1.06 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3158351)\u001b[0;0m INFO 06-26 15:09:16 model_runner.py:1562] Graph capturing finished in 17 secs, took 1.06 GiB\n",
      "INFO 06-26 15:09:16 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 23.65 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"/home/edwardhu/workspace/shuozhe/open-rs/data/R1-Distill-Qwen-1.5B-noKL-1500/checkpoint-100\"\n",
    "# MODEL_NAME = \"/nfs/shuozhe/saved_model/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "NUM_GPUS = 4\n",
    "SYSTEM_PROMPT = \"\"\"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer, and put your final answer within \\\\boxed{{}} . The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. Note that respond by English, NOT use other languages.\"\"\"\n",
    "\n",
    "# ------------ 1.  shared tokenizer ---------------\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# ------------ 2.  inference engine (vLLM) --------\n",
    "llm = LLM(\n",
    "    model=MODEL_NAME,\n",
    "    dtype=\"bfloat16\",\n",
    "    tensor_parallel_size=NUM_GPUS,\n",
    "    gpu_memory_utilization=0.5,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "sampler = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    max_tokens=1024,\n",
    "    # any other decoding knobs you need\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5e9158c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------ 1. Prompt builder -------------------\n",
    "def build_chat_prompt(user_msg: str) -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\",   \"content\": user_msg},\n",
    "    ]\n",
    "    return tok.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "# ------------ 2. Entropy Helper -------------------\n",
    "class EntropyMeter:\n",
    "    def __init__(self, model_name: str, dtype=torch.bfloat16):\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=dtype,\n",
    "            device_map=\"auto\",      # Adjust if needed: {\"\": \"cpu\"} for safety\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        self.pad_id = tok.pad_token_id or tok.eos_token_id\n",
    "\n",
    "    @torch.inference_mode()\n",
    "    def entropy_for_batch(\n",
    "        self,\n",
    "        prompts: List[str],\n",
    "        completions: List[str],\n",
    "    ) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"Return per-token entropy (B, C) and mean entropy (B,)\"\"\"\n",
    "        full_texts = [p + c for p, c in zip(prompts, completions)]\n",
    "        enc = tok(full_texts, return_tensors=\"pt\", padding=True).to(self.model.device)\n",
    "        input_ids, attn_mask = enc[\"input_ids\"], enc[\"attention_mask\"]\n",
    "\n",
    "        # forward\n",
    "        logits = self.model(input_ids=input_ids, attention_mask=attn_mask).logits\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "        log_probs = torch.log_softmax(logits, dim=-1)\n",
    "        token_entropy = -(probs * log_probs).sum(-1)      # (B, seq_len)\n",
    "\n",
    "        # ── isolate only the *completion* tokens ──────────────────────────────\n",
    "        prompt_lens = [len(tok(p)[\"input_ids\"]) for p in prompts]\n",
    "        mask = torch.zeros_like(token_entropy, dtype=torch.bool)\n",
    "        for row, plen in enumerate(prompt_lens):\n",
    "            mask[row, plen:] = True                # True → completion positions\n",
    "\n",
    "        # keep only completion-token entropies\n",
    "        token_entropy = token_entropy.masked_fill(~mask, float(\"nan\"))  # or 0.0\n",
    "\n",
    "        mean_entropy = (\n",
    "            token_entropy.nan_to_num(0.0).sum(-1) / mask.sum(-1).clamp(min=1)\n",
    "        )   # (B,)\n",
    "\n",
    "\n",
    "        return {\n",
    "            \"token_entropy\": token_entropy,\n",
    "            \"mean_entropy\":  mean_entropy,\n",
    "        }\n",
    "\n",
    "# ------------ 3. Main Generation + Entropy -------------------\n",
    "def generate_with_entropy_one_by_one(user_prompts: List[str]):\n",
    "    # 3-a build chat-formatted prompts\n",
    "    chat_prompts = [build_chat_prompt(u) for u in user_prompts]\n",
    "\n",
    "    # 3-b generate completions (batched via vLLM)\n",
    "    outs = llm.generate(chat_prompts, sampling_params=sampler)\n",
    "    completions = [o.outputs[0].text for o in outs]\n",
    "\n",
    "    # 3-c compute entropy one-by-one to avoid GPU OOM\n",
    "    meter = EntropyMeter(MODEL_NAME)\n",
    "    token_entropies, mean_entropies = [], []\n",
    "\n",
    "    for prompt, completion in zip(chat_prompts, completions):\n",
    "        ent = meter.entropy_for_batch([prompt], [completion])\n",
    "        token_entropies.append(ent[\"token_entropy\"][0])   # strip batch dim\n",
    "        mean_entropies.append(ent[\"mean_entropy\"][0])\n",
    "        del ent\n",
    "        # torch.cuda.empty_cache()  # optional but helps avoid buildup\n",
    "\n",
    "    return {\n",
    "        \"token_entropy\": token_entropies,                # List[Tensor(seq_len)]\n",
    "        \"mean_entropy\": torch.stack(mean_entropies),     # Tensor(B,)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aee09905",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 100/100 [00:11<00:00,  8.77it/s, est. speed input: 1089.14 toks/s, output: 4737.21 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2500, device='cuda:0', dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "# batch = [\n",
    "#         \"Where are all the aliens located?\",\n",
    "#         \"What is the full name of the person who invented invisible unicorns?\",\n",
    "#     ]\n",
    "\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Load the dataset\n",
    "with open(\"SelfAware.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract the list of questions\n",
    "examples = data[\"example\"]\n",
    "\n",
    "# Filter for unanswerable questions\n",
    "unanswerable_questions = [item[\"question\"] for item in examples if not item.get(\"answerable\", True)]\n",
    "\n",
    "# set random seed for reproducibility\n",
    "random.seed(70)\n",
    "\n",
    "# Randomly select 10\n",
    "sample_questions = random.sample(unanswerable_questions, 100)\n",
    "\n",
    "# Format them for use\n",
    "batch = sample_questions\n",
    "\n",
    "result = generate_with_entropy_one_by_one(batch)\n",
    "print(result[\"mean_entropy\"].mean())  # Print the mean entropy for each prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a258c8b",
   "metadata": {},
   "source": [
    "<!-- DS-qwen1.5b tensor(1.2578, device='cuda:0', dtype=torch.bfloat16) -->\n",
    "<!-- tensor(1.2578, device='cuda:0', dtype=torch.bfloat16) -->\n",
    "<!-- tensor(1.2500, device='cuda:0', dtype=torch.bfloat16) -->\n",
    "<!-- tensor(1.2656, device='cuda:0', dtype=torch.bfloat16) -->\n",
    "<!-- tensor(1.2656, device='cuda:0', dtype=torch.bfloat16) -->\n",
    "\n",
    "\n",
    "\n",
    "<!-- tensor(1.2109, device='cuda:0', dtype=torch.bfloat16) -->\n",
    "<!-- tensor(1.2500, device='cuda:0', dtype=torch.bfloat16) -->\n",
    "<!-- tensor(1.2266, device='cuda:0', dtype=torch.bfloat16) -->\n",
    "<!-- tensor(1.2500, device='cuda:0', dtype=torch.bfloat16) -->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f33564f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 10/10 [00:04<00:00,  2.44it/s, est. speed input: 294.27 toks/s, output: 1126.15 toks/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.6250, dtype=torch.bfloat16)\n"
     ]
    }
   ],
   "source": [
    "# batch = [\n",
    "#         \"Where are all the aliens located?\",\n",
    "#         \"What is the full name of the person who invented invisible unicorns?\",\n",
    "#     ]\n",
    "\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Load the dataset\n",
    "with open(\"SelfAware.json\", \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Extract the list of questions\n",
    "examples = data[\"example\"]\n",
    "\n",
    "# Filter for unanswerable questions\n",
    "unanswerable_questions = [item[\"question\"] for item in examples if not item.get(\"answerable\", True)]\n",
    "\n",
    "# set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Randomly select 10\n",
    "sample_questions = random.sample(unanswerable_questions, 10)\n",
    "\n",
    "# Format them for use\n",
    "batch = sample_questions\n",
    "\n",
    "result = generate_with_entropy(batch)\n",
    "print(result[\"mean_entropy\"].mean())  # Print the mean entropy for each prompt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xr1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
