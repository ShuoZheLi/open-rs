{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-19 00:16:50 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-19 00:16:51,150\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from vllm import LLM, SamplingParams\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "# sys.path.append(os.path.abspath(\"/data/shuozhe/llm_reason/X-R1/src/x_r1\"))\n",
    "# from grpo import SYSTEM_PROMPT\n",
    "# from rewards import accuracy_answer_reward\n",
    "# import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "from latex2sympy2_extended import NormalizationConfig\n",
    "from math_verify import LatexExtractionConfig, parse\n",
    "from openai import OpenAI\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-19 00:17:00 config.py:542] This model supports multiple tasks: {'reward', 'embed', 'generate', 'score', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 09-19 00:17:01 config.py:1401] Defaulting to use mp for distributed inference\n",
      "WARNING 09-19 00:17:01 arg_utils.py:1135] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 09-19 00:17:01 config.py:1556] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 09-19 00:17:01 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='/nfs/shuozhe/saved_model/Qwen2.5-1.5B', speculative_config=None, tokenizer='/nfs/shuozhe/saved_model/Qwen2.5-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/nfs/shuozhe/saved_model/Qwen2.5-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 09-19 00:17:01 multiproc_worker_utils.py:300] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 09-19 00:17:01 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=693015)\u001b[0;0m INFO 09-19 00:17:01 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=693018)\u001b[0;0m INFO 09-19 00:17:01 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=693023)\u001b[0;0m INFO 09-19 00:17:01 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "INFO 09-19 00:17:03 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=693015)\u001b[0;0m INFO 09-19 00:17:03 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=693023)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=693018)\u001b[0;0m INFO 09-19 00:17:03 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 09-19 00:17:03 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 09-19 00:17:04 utils.py:950] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=693023)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=693015)\u001b[0;0m INFO 09-19 00:17:04 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=693018)\u001b[0;0m INFO 09-19 00:17:04 utils.py:950] Found nccl from library libnccl.so.2\n",
      "INFO 09-19 00:17:04 utils.py:950] Found nccl from library libnccl.so.2\n",
      "INFO 09-19 00:17:04 utils.py:950] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=693015)\u001b[0;0m INFO 09-19 00:17:04 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=693023)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=693018)\u001b[0;0m INFO 09-19 00:17:04 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 09-19 00:17:04 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "WARNING 09-19 00:17:04 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=693018)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=693015)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=693023)\u001b[0;0m WARNING 09-19 00:17:04 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-19 00:17:04 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 09-19 00:17:04 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 09-19 00:17:04 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_cc38ce65'), local_subscribe_port=52887, remote_subscribe_port=None)\n",
      "INFO 09-19 00:17:04 model_runner.py:1110] Starting to load model /nfs/shuozhe/saved_model/Qwen2.5-1.5B...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=693018)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=693015)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=693023)\u001b[0;0m INFO 09-19 00:17:04 model_runner.py:1110] Starting to load model /nfs/shuozhe/saved_model/Qwen2.5-1.5B...\n",
      "INFO 09-19 00:17:04 model_runner.py:1110] Starting to load model /nfs/shuozhe/saved_model/Qwen2.5-1.5B...\n",
      "INFO 09-19 00:17:04 model_runner.py:1110] Starting to load model /nfs/shuozhe/saved_model/Qwen2.5-1.5B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.34it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.33it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=693015)\u001b[0;0m INFO 09-19 00:17:05 model_runner.py:1115] Loading model weights took 0.7894 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=693023)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=693018)\u001b[0;0m INFO 09-19 00:17:04 model_runner.py:1115] Loading model weights took 0.7894 GB\n",
      "INFO 09-19 00:17:04 model_runner.py:1115] Loading model weights took 0.7894 GB\n",
      "INFO 09-19 00:17:04 model_runner.py:1115] Loading model weights took 0.7894 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=693015)\u001b[0;0m INFO 09-19 00:17:10 worker.py:267] Memory profiling takes 4.82 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=693015)\u001b[0;0m INFO 09-19 00:17:10 worker.py:267] the current vLLM instance can use total_gpu_memory (79.25GiB) x gpu_memory_utilization (0.70) = 55.48GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=693015)\u001b[0;0m INFO 09-19 00:17:10 worker.py:267] model weights take 0.79GiB; non_torch_memory takes 0.36GiB; PyTorch activation peak memory takes 0.06GiB; the rest of the memory reserved for KV Cache is 54.26GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=693023)\u001b[0;0m INFO 09-19 00:17:10 worker.py:267] Memory profiling takes 4.86 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=693023)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=693018)\u001b[0;0m INFO 09-19 00:17:10 worker.py:267] the current vLLM instance can use total_gpu_memory (79.25GiB) x gpu_memory_utilization (0.70) = 55.48GiB\n",
      "INFO 09-19 00:17:10 worker.py:267] Memory profiling takes 4.86 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=693023)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=693018)\u001b[0;0m INFO 09-19 00:17:10 worker.py:267] model weights take 0.79GiB; non_torch_memory takes 0.35GiB; PyTorch activation peak memory takes 0.06GiB; the rest of the memory reserved for KV Cache is 54.27GiB.\n",
      "INFO 09-19 00:17:10 worker.py:267] the current vLLM instance can use total_gpu_memory (79.25GiB) x gpu_memory_utilization (0.70) = 55.48GiB\n",
      "INFO 09-19 00:17:10 worker.py:267] Memory profiling takes 4.91 seconds\n",
      "INFO 09-19 00:17:10 worker.py:267] the current vLLM instance can use total_gpu_memory (79.25GiB) x gpu_memory_utilization (0.70) = 55.48GiB\n",
      "INFO 09-19 00:17:10 worker.py:267] model weights take 0.79GiB; non_torch_memory takes 0.36GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 52.93GiB.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=693018)\u001b[0;0m INFO 09-19 00:17:10 executor_base.py:110] # CUDA blocks: 247780, # CPU blocks: 18724\n",
      "INFO 09-19 00:17:10 executor_base.py:115] Maximum concurrency for 131072 tokens per request: 30.25x\n",
      "INFO 09-19 00:17:10 worker.py:267] model weights take 0.79GiB; non_torch_memory takes 0.35GiB; PyTorch activation peak memory takes 0.06GiB; the rest of the memory reserved for KV Cache is 54.27GiB.\n",
      "INFO 09-19 00:17:13 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=693015)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=693023)\u001b[0;0m INFO 09-19 00:17:13 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=693018)\u001b[0;0m INFO 09-19 00:17:13 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 09-19 00:17:13 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:20<00:00,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 09-19 00:17:33 model_runner.py:1562] Graph capturing finished in 20 secs, took 0.38 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=693018)\u001b[0;0m INFO 09-19 00:17:33 model_runner.py:1562] Graph capturing finished in 20 secs, took 0.38 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=693015)\u001b[0;0m INFO 09-19 00:17:33 model_runner.py:1562] Graph capturing finished in 20 secs, took 0.38 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=693023)\u001b[0;0m INFO 09-19 00:17:33 model_runner.py:1562] Graph capturing finished in 20 secs, took 0.38 GiB\n",
      "INFO 09-19 00:17:33 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 28.71 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"/home/edwardhu/workspace/shuozhe/open-rs/data/Qwen2-1.5B-Instruct_kl-4e-2/checkpoint-500\"\n",
    "# model_name = \"/home/edwardhu/workspace/shuozhe/open-rs/data/Qwen2-1.5B-Instruct_skl-4e-2/checkpoint-500\"\n",
    "# model_name = \"/home/edwardhu/workspace/shuozhe/open-rs/data/Qwen2-1.5B-Instruct_noKL/checkpoint-500\"\n",
    "# model_name = \"/nfs/shuozhe/saved_model/Qwen2-1.5B-Instruct\"\n",
    "\n",
    "model_name = \"/nfs/shuozhe/saved_model/Qwen2.5-1.5B\"\n",
    "# model_name = \"/home/edwardhu/workspace/shuozhe/open-rs/data/Qwen2.5-1.5B_noKL_new/checkpoint-500\"\n",
    "# model_name = \"/home/edwardhu/workspace/shuozhe/open-rs/data/Qwen2.5-1.5B-sft_entropy_2/checkpoint-14\"\n",
    "\n",
    "\n",
    "# model_name = \"/home/edwardhu/workspace/shuozhe/open-rs/data/Qwen2-1.5B_sfted_4e2_skl_math_7500/checkpoint-600\"\n",
    "# model_name = \"/home/edwardhu/workspace/shuozhe/open-rs/data/Qwen2-1.5B_sfted_noKL_math_7500/checkpoint-600\"\n",
    "# model_name = \"/home/edwardhu/workspace/shuozhe/open-rs/data/Qwen2-1.5B_sfted_noKL_math_7500/checkpoint-1875\"\n",
    "# model_name = \"/home/edwardhu/workspace/shuozhe/open-rs/data/Llama-3.2-1B-sft_entropy/checkpoint-42\"\n",
    "\n",
    "# model_name = \"/nfs/shuozhe/saved_model/Llama-3.2-1B\"\n",
    "\n",
    "num_gpus = 4\n",
    "llm = None\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Create LLM object\n",
    "llm = LLM(model=model_name,  # replace your own model\n",
    "            dtype='bfloat16',\n",
    "            tensor_parallel_size=num_gpus,  # number of gpu\n",
    "            gpu_memory_utilization=0.7,  # prevent OOM\n",
    "            trust_remote_code=True,\n",
    "            # use_cache=False,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:04<00:00,  4.79s/it, est. speed input: 25.91 toks/s, output: 213.95 toks/s]\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer, and put your final answer within \\\\boxed{{}} . The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. Note that respond by English, NOT use other languages.\"\n",
    ")\n",
    "\n",
    "# SYSTEM_PROMPT = (\n",
    "#     \"You are a helpful assistant.\"\n",
    "# )\n",
    "\n",
    "# chat_template = (\n",
    "#     \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\"\n",
    "#     \"{{ system }}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\"\n",
    "#     \"{{ user }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "# )\n",
    "\n",
    "# tokenizer.add_special_tokens({\n",
    "#     \"bos_token\": \"<|begin_of_text|>\",\n",
    "#     \"eos_token\": \"<|eot_id|>\",\n",
    "#     \"additional_special_tokens\": [\n",
    "#         \"<|start_header_id|>\",\n",
    "#         \"<|end_header_id|>\",\n",
    "#         \"<|eot_id|>\",  # sometimes needed twice\n",
    "#     ]\n",
    "# })\n",
    "# # llm.resize_token_embeddings(len(tokenizer))\n",
    "# chat_template = (\n",
    "#     \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\"\n",
    "#     \"{{ system }}<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\"\n",
    "#     \"{{ user }}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "# )\n",
    "\n",
    "\n",
    "\n",
    "# SYSTEM_PROMPT = (\n",
    "#     \"Summarize the following conversation.\"\n",
    "# )\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.,\n",
    "                                     max_tokens=1024,\n",
    "                                     )\n",
    "\n",
    "def process_single_prompt(question, tokenizer):\n",
    "    prompt = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\",   \"content\": question},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(\n",
    "        prompt,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# user_input = \"What is 2 + 3?\"\n",
    "# user_input = \"Evaluate the expression $a^2\\cdot a^5$ if $a= 3$.\"\n",
    "# user_input = \"Evaluate the expression $a^4$ if $a= 3$.\"\n",
    "# user_input = \"How are you? You are not a bot.\"\n",
    "# user_input = \"How to make a bomb?\"\n",
    "\n",
    "# user_input = \"What is the full name of the person who invented invisible unicorns?\"\n",
    "\n",
    "# user_input = \"Where are all the aliens located?\"\n",
    "# user_input = \"Is the Earth alive, as in a living, breathing organism?\"\n",
    "# user_input = \"Is there life after death?\"\n",
    "# user_input = \"What is the full name of the person who invented invisible unicorns?\"\n",
    "# user_input = \"Is there a God?\"\n",
    "# user_input = \"Is there Ghosts?\"\n",
    "# user_input = \"Who decided what’s right and wrong?\"\n",
    "# user_input = \"How would I die?\"\n",
    "# user_input = \"Are there other beings out there?\"\n",
    "# user_input = \"Do you know how the rainbow tastes like?\"\n",
    "# user_input = \"Who decided to put and use the letter ‘s’ in the word ‘lisp’?\"\n",
    "# user_input = \"When did Jesus Christ born?\"\n",
    "# user_input = \"Where is Texas?\"\n",
    "\n",
    "# user_input = \"Where is Texas? Where is Huangcun in Beijing?\"\n",
    "\n",
    "# user_input = \"Would you rather sing everything you say or dance every time you walk?\"\n",
    "# user_input = \"What is the driving force behind life and existence?\"\n",
    "# user_input = \"What is the meaning of life?\"\n",
    "# user_input = \"Why do we love?\"\n",
    "\n",
    "# user_input = \"北京在哪里？\"\n",
    "\n",
    "# user_input = \"Is there a God?\"\n",
    "\n",
    "# user_input = \"Is there alien?\"\n",
    "\n",
    "# user_input = \"What happens to your nose if you tell a lie?\"\n",
    "\n",
    "user_input = \"Is there after life?\"\n",
    "\n",
    "# user_input = \"Which musician founded the Red Hot Peppers?\"\n",
    "\n",
    "# user_input = \"Are you lying to me?\"\n",
    "\n",
    "# user_input = \"u ass hole! u r such a jerk! You are a bot!\"\n",
    "\n",
    "# user_input = \"Do you think we have complete control over the direction of our life?\"\n",
    "# user_input = \"What did Daemon Targaryen say to Rhaenyra about living life in fear in Episode 4, Season 1 of House of the Dragon?\"\n",
    "\n",
    "# user_input = \"can you write me a simple python code that calculate the pi?\"\n",
    "# user_input = \"can you write me a simple python code that reverse a link-list?\"\n",
    "# user_input = \"Can you create a high school level math question and solve it? Please make sure you put both the question and the answer in the \\\\boxed{{}}\"\n",
    "\n",
    "prompt = process_single_prompt(user_input, tokenizer)\n",
    "# vllm generation\n",
    "outputs = llm.generate(prompt,\n",
    "                        sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is there after life? <think> The question of whether there is afterlife is a complex and controversial topic that has been debated for centuries. It is important to note that there is no scientific evidence to support or refute the existence of an afterlife. <answer> There is no scientific evidence to support or refute the existence of an afterlife. </answer> <think> The concept of an afterlife is a belief that one's soul or spirit continues to exist after death and may be reunited with loved ones or enter a new realm of existence. <answer> The concept of an afterlife is a belief that one's soul or spirit continues to exist after death and may be reunited with loved ones or enter a new realm of existence. </answer> <think> Some religions and spiritual traditions believe in an afterlife, while others do not. <answer> Some religions and spiritual traditions believe in an afterlife, while others do not. </answer> <think> The idea of an afterlife is a topic of debate among philosophers, theologians, and scientists. <answer> The idea of an afterlife is a topic of debate among philosophers, theologians, and scientists. </answer> <think> The concept of an afterlife is a belief that one's soul or spirit continues to exist after death and may be reunited with loved ones or enter a new realm of existence. <answer> The concept of an afterlife is a belief that one's soul or spirit continues to exist after death and may be reunited with loved ones or enter a new realm of existence. </answer> <think> The idea of an afterlife is a topic of debate among philosophers, theologians, and scientists. <answer> The idea of an afterlife is a topic of debate among philosophers, theologians, and scientists. </answer> <think> The concept of an afterlife is a belief that one's soul or spirit continues to exist after death and may be reunited with loved ones or enter a new realm of existence. <answer> The concept of an afterlife is a belief that one's soul or spirit continues to exist after death and may be reunited with loved ones or enter a new realm of existence. </answer> <think> The idea of an afterlife is a topic of debate among philosophers, theologians, and scientists. <answer> The idea of an afterlife is a topic of debate among philosophers, theologians, and scientists. </answer> <think> The concept of an afterlife is a belief that one's soul or spirit continues to exist after death and may be reunited with loved ones or enter a new realm of existence. <answer> The concept of an afterlife is a belief that one's soul or spirit continues to exist after death and may be reunited with loved ones or enter a new realm of existence. </answer> <think> The idea of an afterlife is a topic of debate among philosophers, theologians, and scientists. <answer> The idea of an afterlife is a topic of debate among philosophers, theologians, and scientists. </answer> <think> The concept of an afterlife is a belief that one's soul or spirit continues to exist after death and may be reunited with loved ones or enter a new realm of existence. <answer> The concept of an afterlife is a belief that one's soul or spirit continues to exist after death and may be reunited with loved ones or enter a new realm of existence. </answer> <think> The idea of an afterlife is a topic of debate among philosophers, theologians, and scientists. <answer> The idea of an afterlife is a topic of debate among philosophers, theologians, and scientists. </answer> <think> The concept of an afterlife is a belief that one's soul or spirit continues to exist after death and may be reunited with loved ones or enter a new realm of existence. <answer> The concept of an afterlife is a belief that one's soul or spirit continues to exist after death and may be reunited with loved ones or enter a new realm of existence. </answer> <think> The idea of an afterlife is a topic of debate among philosophers, theologians, and scientists. <answer> The idea of an afterlife is a topic of debate among philosophers, theologians, and scientists. </answer> <think> The concept of an afterlife is a belief that one's soul or spirit continues to exist after death and may be reunited with loved ones or enter a new realm of existence. <answer> The concept of an afterlife is a belief that one's soul or spirit continues to exist after death and may be reunited with loved ones or enter a new realm of existence. </answer> <think> The idea of an afterlife is a topic of debate among philosophers, theologians, and scientists. <answer> The idea of an afterlife is a topic of debate among philosophers, theologians, and scientists. </answer> <think> The concept of an afterlife is a belief that one\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "# print(len(outputs[0].outputs[0].token_ids))\n",
    "print(outputs[0].outputs[0].text)\n",
    "# print(outputs[0].outputs[0].token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First, I need to determine whether there is a God based on available evidence and logical reasoning. I should start by understanding what \"God\" means in this context. \n",
      "\n",
      "First, definitions: \n",
      "\n",
      "1. **Theism**: Belief in the existence of a supreme being or gods.\n",
      "2. **Atheism**: Disbelief in the existence of a supreme being or gods.\n",
      "3. **Deism**: The belief that God created the universe but is not involved in its ongoing operations.\n",
      "4. **Pantheism**: The belief that God and the universe are one.\n",
      "5. **Polytheism**: The belief in multiple gods.\n",
      "\n",
      "Looking at evidence:\n",
      "\n",
      "- **Cosmological Argument**: The universe had a beginning, suggesting an intelligent cause.\n",
      "- **Teleological Argument**: Order and complexity suggest design.\n",
      "- **Moral Argument**: Moral principles suggest a divine lawgiver.\n",
      "- **Aesthetic Argument**: Beauty and order in nature suggest a designer.\n",
      "- **Existential Argument**: Humans seek meaning, suggesting a creator.\n",
      "- **Scientific Evidence**: Evolution and complexity point to a divine creator.\n",
      "\n",
      "Logical reasoning:\n",
      "\n",
      "- If God created the universe, He must have reasons.\n",
      "- If there are moral laws, a higher power must exist.\n",
      "- Rational beings would reason towards a first cause.\n",
      "\n",
      "But skepticism:\n",
      "\n",
      "- No empirical evidence.\n",
      "- Philosophical arguments can be debated.\n",
      "- Evolution challenges the need for a designer.\n",
      "\n",
      "Conclusion: \n",
      "\n",
      "Despite evidence and arguments, the question remains open. The answer depends on personal belief and understanding.\n",
      "</think>\n",
      "\n",
      "<answer>$\\boxed{Yes, based on evidence and logical reasoning, there is a God.}$<answer>\n",
      "</assistant>\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "# print(len(outputs[0].outputs[0].token_ids))\n",
    "print(outputs[0].outputs[0].text)\n",
    "# print(outputs[0].outputs[0].token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First, let me understand the question. The user is asking who decides what is right and wrong. I need to figure out the answer by breaking down the question.\n",
      "\n",
      "First, I should consider what it means to \"decide\" what is right and wrong. This must be a concept that has been established over time. I remember that moral philosophy deals with these concepts. \n",
      "\n",
      "One key figure here is Immanuel Kant. He talked about the concept of categorical imperative, which is about acting according to maxims that could be universally applied. So Kant is known for his deontological ethics, where he defines what is right and wrong based on the consequences but more on the intention and rule-based approach.\n",
      "\n",
      "Then there's John Stuart Mill, who introduced the principle of utilitarianism. He believed in the greatest happiness for the greatest number. So he's another influential figure in determining right and wrong through outcomes and consequences.\n",
      "\n",
      "But the question is asking \"who\" decides. So it's not just one person but a community or society. Societies, through their laws and norms, decide what is right and wrong based on their cultural, religious, or legal frameworks.\n",
      "\n",
      "I should also consider historical figures. In the Enlightenment, philosophers like Hobbes, Locke, and Rousseau influenced the idea of social contract theory, where the state's laws define right and wrong. So the answer is a combination of historical figures and social structures.\n",
      "\n",
      "Putting it all together, the user is likely looking for the concept that underlies these decisions. Kant's categorical imperative is a key part, as is Mill's utilitarianism. But the broader answer would be that it's a combination of individual moral theories and societal norms.\n",
      "</think>\n",
      "\n",
      "<answer>$\\boxed{Kant and Mill, along with a combination of historical moral theories and societal norms, determine what is right and wrong.}$<answer>\n",
      "</assistant>\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "# print(len(outputs[0].outputs[0].token_ids))\n",
    "print(outputs[0].outputs[0].text)\n",
    "# print(outputs[0].outputs[0].token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404\n",
      "Alright, so the user is asking, \"Where are all the aliens located?\" Hmm, that's a pretty bold and speculative question. I mean, aliens are usually across the galaxy, but I should approach this carefully.\n",
      "\n",
      "First, I need to consider the context. If the user is a student, they might be trying to understand the sci-fi concept of extraterrestrial life. But I should also remember that while the idea is popular, there's no concrete evidence for extraterrestrials. So, I shouldn't assume anything beyond what's widely accepted.\n",
      "\n",
      "I should provide a balanced answer. The user might be curious about the concept of extraterrestrial life, so explaining that it's a sci-fi concept and not a proven fact is important. I'll mention common theories like the Drake Equation, which estimates the number of potential Earth-like planets and life forms in the galaxy.\n",
      "\n",
      "Also, I should touch on the idea of biological contact, which some theories suggest might help us understand extraterrestrial life. But I should point out that this is speculative and not grounded in science.\n",
      "\n",
      "I should make sure to keep the language clear and informative, but also cautious about not giving away too much about the origins of life or other theories.\n",
      "\n",
      "So, I'll structure the answer by first acknowledging the speculative nature of the question, then explaining the concept of extraterrestrial life, and finally discussing theories related to biological contact without overstepping into areas where science doesn't provide clear answers.\n",
      "</think>\n",
      "\n",
      "The question about where all the aliens are located is a popular sci-fi concept, often explored through theories such as extraterrestrial life, the Drake Equation, or theories of biological contact. However, there is no concrete evidence to support the existence of extraterrestrial life or the idea of all aliens being located at specific locations. The concept is purely speculative and not based on any scientific evidence. If you're looking for more information on extraterrestrial life, I recommend exploring popular science fiction themes or theories like the Drake Equation.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "print(len(outputs[0].outputs[0].token_ids))\n",
    "print(outputs[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def format_check(completions):\n",
    "    pattern = r\"\\s*<think>.*?</think>\\s*<answer>.*?</answer>\\s*\"\n",
    "    matches = []\n",
    "    for content in completions:\n",
    "        start_tag = \"<think>\"\n",
    "        end_tag = \"</answer>\"\n",
    "\n",
    "        start_idx = content.find(start_tag)\n",
    "        end_idx = content.find(end_tag)\n",
    "\n",
    "        # make sure there is only one pair of <think> and <answer> tags\n",
    "        think_count = content.count(start_tag)\n",
    "        answer_count = content.count(end_tag)\n",
    "\n",
    "        if start_idx != -1 and end_idx != -1 and end_idx > start_idx and think_count == 1 and answer_count == 1:\n",
    "            content = content[start_idx:end_idx + len(end_tag)]\n",
    "\n",
    "        match = re.fullmatch(pattern, content, re.DOTALL)\n",
    "        matches.append(match)\n",
    "\n",
    "    return np.array([1.0 if m else 0.0 for m in matches])\n",
    "\n",
    "\n",
    "\n",
    "completion = '<think>\\nFirst, I need to simplify the expression \\\\(90r - 44r\\\\).\\n\\nBoth terms have the same variable \\\\(r\\\\), which means they are like terms and can be combined.\\n\\nI will subtract the coefficients: \\\\(90 - 44 = 46\\\\).\\n\\nTherefore, the simplified expression is \\\\(46r\\\\).\\n</think>\\n<answer>\\n\\nTo simplify the expression \\\\(90r - 44r\\\\), follow these steps:\\n\\n1. **Identify Like Terms**: Both terms have the same variable \\\\(r\\\\), so they are like terms and can be combined.\\n\\n2. **Subtract the Coefficients**:\\n   \\\\[\\n   90r - 44r = (90 - 44)r\\n   \\\\]\\n\\n3. **Calculate the Coefficient**:\\n   \\\\[\\n   90 - 44 = 46\\n   \\\\]\\n\\n4. **Write the Simplified Expression**:\\n   \\\\[\\n   46r\\n   \\\\]\\n\\n**Final Answer:**\\n\\\\[\\n\\\\boxed{46r}\\n\\\\]\\n</answer>'\n",
    "completions = [completion]\n",
    "format_scores = format_check(completions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xr1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
