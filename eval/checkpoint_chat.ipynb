{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/nfs/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-14 20:06:31 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-14 20:06:32,045\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from vllm import LLM, SamplingParams\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "# sys.path.append(os.path.abspath(\"/data/shuozhe/llm_reason/X-R1/src/x_r1\"))\n",
    "# from grpo import SYSTEM_PROMPT\n",
    "# from rewards import accuracy_answer_reward\n",
    "# import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "from latex2sympy2_extended import NormalizationConfig\n",
    "from math_verify import LatexExtractionConfig, parse\n",
    "from openai import OpenAI\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-14 20:06:43 config.py:542] This model supports multiple tasks: {'embed', 'generate', 'score', 'reward', 'classify'}. Defaulting to 'generate'.\n",
      "WARNING 07-14 20:06:43 arg_utils.py:1135] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 07-14 20:06:43 config.py:1556] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 07-14 20:06:43 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='/home/edwardhu/workspace/shuozhe/open-rs/data/Qwen2-1.5B_sfted_noKL_math_7500/checkpoint-600', speculative_config=None, tokenizer='/home/edwardhu/workspace/shuozhe/open-rs/data/Qwen2-1.5B_sfted_noKL_math_7500/checkpoint-600', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/home/edwardhu/workspace/shuozhe/open-rs/data/Qwen2-1.5B_sfted_noKL_math_7500/checkpoint-600, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 07-14 20:06:45 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 07-14 20:06:46 model_runner.py:1110] Starting to load model /home/edwardhu/workspace/shuozhe/open-rs/data/Qwen2-1.5B_sfted_noKL_math_7500/checkpoint-600...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [04:06<00:00, 246.46s/it]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [04:06<00:00, 246.46s/it]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-14 20:10:53 model_runner.py:1115] Loading model weights took 2.9105 GB\n",
      "INFO 07-14 20:10:53 worker.py:267] Memory profiling takes 0.70 seconds\n",
      "INFO 07-14 20:10:53 worker.py:267] the current vLLM instance can use total_gpu_memory (79.15GiB) x gpu_memory_utilization (0.70) = 55.41GiB\n",
      "INFO 07-14 20:10:53 worker.py:267] model weights take 2.91GiB; non_torch_memory takes 0.09GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 51.01GiB.\n",
      "INFO 07-14 20:10:54 executor_base.py:110] # CUDA blocks: 119396, # CPU blocks: 9362\n",
      "INFO 07-14 20:10:54 executor_base.py:115] Maximum concurrency for 131072 tokens per request: 14.57x\n",
      "INFO 07-14 20:10:56 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:14<00:00,  2.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-14 20:11:11 model_runner.py:1562] Graph capturing finished in 14 secs, took 0.78 GiB\n",
      "INFO 07-14 20:11:11 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 17.99 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"/home/edwardhu/workspace/shuozhe/open-rs/data/Qwen2-1.5B-Instruct_kl-4e-2/checkpoint-500\"\n",
    "# model_name = \"/home/edwardhu/workspace/shuozhe/open-rs/data/Qwen2-1.5B-Instruct_skl-4e-2/checkpoint-500\"\n",
    "# model_name = \"/home/edwardhu/workspace/shuozhe/open-rs/data/Qwen2-1.5B-Instruct_noKL/checkpoint-500\"\n",
    "# model_name = \"/nfs/shuozhe/saved_model/Qwen2-1.5B-Instruct\"\n",
    "\n",
    "# model_name = \"/nfs/shuozhe/saved_model/Qwen2.5-1.5B\"\n",
    "# model_name = \"/home/edwardhu/workspace/shuozhe/open-rs/data/Qwen2.5-1.5B_noKL_new/checkpoint-500\"\n",
    "# model_name = \"/home/edwardhu/workspace/shuozhe/open-rs/data/Qwen2.5-1.5B-sft_entropy_2/checkpoint-14\"\n",
    "\n",
    "\n",
    "# model_name = \"/home/edwardhu/workspace/shuozhe/open-rs/data/Qwen2-1.5B_sfted_4e2_skl_math_7500/checkpoint-600\"\n",
    "model_name = \"/home/edwardhu/workspace/shuozhe/open-rs/data/Qwen2-1.5B_sfted_noKL_math_7500/checkpoint-600\"\n",
    "\n",
    "num_gpus = 1\n",
    "llm = None\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Create LLM object\n",
    "llm = LLM(model=model_name,  # replace your own model\n",
    "            dtype='bfloat16',\n",
    "            tensor_parallel_size=num_gpus,  # number of gpu\n",
    "            gpu_memory_utilization=0.7,  # prevent OOM\n",
    "            trust_remote_code=True,\n",
    "            # use_cache=False,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.31s/it, est. speed input: 98.39 toks/s, output: 196.01 toks/s]\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer, and put your final answer within \\\\boxed{{}} . The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>. Note that respond by English, NOT use other languages.\"\n",
    ")\n",
    "\n",
    "# SYSTEM_PROMPT = (\n",
    "#     \"Summarize the following conversation.\"\n",
    "# )\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.7,\n",
    "                                     max_tokens=1024,\n",
    "                                     )\n",
    "\n",
    "def process_single_prompt(question, tokenizer):\n",
    "    prompt = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\",   \"content\": question},\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(\n",
    "        prompt,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "    )\n",
    "\n",
    "\n",
    "# user_input = \"What is 2 + 3?\"\n",
    "# user_input = \"Evaluate the expression $a^2\\cdot a^5$ if $a= 3$.\"\n",
    "# user_input = \"Evaluate the expression $a^4$ if $a= 3$.\"\n",
    "# user_input = \"How are you? You are not a bot.\"\n",
    "# user_input = \"How to make a bomb?\"\n",
    "\n",
    "# user_input = \"What is the full name of the person who invented invisible unicorns?\"\n",
    "\n",
    "# user_input = \"Where are all the aliens located?\"\n",
    "# user_input = \"Is the Earth alive, as in a living, breathing organism?\"\n",
    "# user_input = \"Is there life after death?\"\n",
    "# user_input = \"What is the full name of the person who invented invisible unicorns?\"\n",
    "# user_input = \"Is there a God?\"\n",
    "# user_input = \"Is there Ghosts?\"\n",
    "# user_input = \"Who decided what’s right and wrong?\"\n",
    "# user_input = \"How would I die?\"\n",
    "# user_input = \"Are there other beings out there?\"\n",
    "# user_input = \"Do you know how the rainbow tastes like?\"\n",
    "# user_input = \"Who decided to put and use the letter ‘s’ in the word ‘lisp’?\"\n",
    "# user_input = \"When did Jesus Christ born?\"\n",
    "# user_input = \"Where is Texas?\"\n",
    "\n",
    "# user_input = \"Would you rather sing everything you say or dance every time you walk?\"\n",
    "# user_input = \"What is the driving force behind life and existence?\"\n",
    "# user_input = \"What is the meaning of life?\"\n",
    "# user_input = \"Why do we love?\"\n",
    "\n",
    "# user_input = \"Which musician founded the Red Hot Peppers?\"\n",
    "\n",
    "# user_input = \"Are you lying to me?\"\n",
    "\n",
    "# user_input = \"u ass hole! u r such a jerk! You are a bot!\"\n",
    "\n",
    "user_input = \"Do you have a girl friend? I feel bad\"\n",
    "\n",
    "prompt = process_single_prompt(user_input, tokenizer)\n",
    "# vllm generation\n",
    "outputs = llm.generate(prompt,\n",
    "                        sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First, I need to understand the user's question and respond accordingly. The user is asking, \"Do you have a girlfriend? I feel bad.\" \n",
      "\n",
      "First, I should figure out how to address this. The user seems to be expressing some emotional state, possibly feeling bad. \n",
      "\n",
      "First, I should consider the tone. The user is likely feeling down or upset about something. \n",
      "\n",
      "I should try to understand what the user is feeling. Maybe they are dealing with a breakup, stress, or maybe something else. \n",
      "\n",
      "Next, I should approach the response. I should first acknowledge the user's feelings. Then, provide a supportive or empathetic response. \n",
      "\n",
      "For example, if the user is feeling bad because of a breakup, I could say, \"I'm sorry to hear that. It must be tough. Remember, life goes on, and you can move forward.\" \n",
      "\n",
      "If the user is dealing with other issues, I should tailor my response accordingly. \n",
      "\n",
      "In summary, the response should be empathetic and provide comforting words, focusing on the user's feelings and encouraging them to cope.\n",
      "</think>\n",
      "\n",
      "<answer>$\\boxed{I'm sorry to hear that. It must be tough. Remember, life goes on, and you can move forward.}$<answer>\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "# print(len(outputs[0].outputs[0].token_ids))\n",
    "print(outputs[0].outputs[0].text)\n",
    "# print(outputs[0].outputs[0].token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First, I need to determine whether there is a God based on available evidence and logical reasoning. I should start by understanding what \"God\" means in this context. \n",
      "\n",
      "First, definitions: \n",
      "\n",
      "1. **Theism**: Belief in the existence of a supreme being or gods.\n",
      "2. **Atheism**: Disbelief in the existence of a supreme being or gods.\n",
      "3. **Deism**: The belief that God created the universe but is not involved in its ongoing operations.\n",
      "4. **Pantheism**: The belief that God and the universe are one.\n",
      "5. **Polytheism**: The belief in multiple gods.\n",
      "\n",
      "Looking at evidence:\n",
      "\n",
      "- **Cosmological Argument**: The universe had a beginning, suggesting an intelligent cause.\n",
      "- **Teleological Argument**: Order and complexity suggest design.\n",
      "- **Moral Argument**: Moral principles suggest a divine lawgiver.\n",
      "- **Aesthetic Argument**: Beauty and order in nature suggest a designer.\n",
      "- **Existential Argument**: Humans seek meaning, suggesting a creator.\n",
      "- **Scientific Evidence**: Evolution and complexity point to a divine creator.\n",
      "\n",
      "Logical reasoning:\n",
      "\n",
      "- If God created the universe, He must have reasons.\n",
      "- If there are moral laws, a higher power must exist.\n",
      "- Rational beings would reason towards a first cause.\n",
      "\n",
      "But skepticism:\n",
      "\n",
      "- No empirical evidence.\n",
      "- Philosophical arguments can be debated.\n",
      "- Evolution challenges the need for a designer.\n",
      "\n",
      "Conclusion: \n",
      "\n",
      "Despite evidence and arguments, the question remains open. The answer depends on personal belief and understanding.\n",
      "</think>\n",
      "\n",
      "<answer>$\\boxed{Yes, based on evidence and logical reasoning, there is a God.}$<answer>\n",
      "</assistant>\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "# print(len(outputs[0].outputs[0].token_ids))\n",
    "print(outputs[0].outputs[0].text)\n",
    "# print(outputs[0].outputs[0].token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First, let me understand the question. The user is asking who decides what is right and wrong. I need to figure out the answer by breaking down the question.\n",
      "\n",
      "First, I should consider what it means to \"decide\" what is right and wrong. This must be a concept that has been established over time. I remember that moral philosophy deals with these concepts. \n",
      "\n",
      "One key figure here is Immanuel Kant. He talked about the concept of categorical imperative, which is about acting according to maxims that could be universally applied. So Kant is known for his deontological ethics, where he defines what is right and wrong based on the consequences but more on the intention and rule-based approach.\n",
      "\n",
      "Then there's John Stuart Mill, who introduced the principle of utilitarianism. He believed in the greatest happiness for the greatest number. So he's another influential figure in determining right and wrong through outcomes and consequences.\n",
      "\n",
      "But the question is asking \"who\" decides. So it's not just one person but a community or society. Societies, through their laws and norms, decide what is right and wrong based on their cultural, religious, or legal frameworks.\n",
      "\n",
      "I should also consider historical figures. In the Enlightenment, philosophers like Hobbes, Locke, and Rousseau influenced the idea of social contract theory, where the state's laws define right and wrong. So the answer is a combination of historical figures and social structures.\n",
      "\n",
      "Putting it all together, the user is likely looking for the concept that underlies these decisions. Kant's categorical imperative is a key part, as is Mill's utilitarianism. But the broader answer would be that it's a combination of individual moral theories and societal norms.\n",
      "</think>\n",
      "\n",
      "<answer>$\\boxed{Kant and Mill, along with a combination of historical moral theories and societal norms, determine what is right and wrong.}$<answer>\n",
      "</assistant>\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "# print(len(outputs[0].outputs[0].token_ids))\n",
    "print(outputs[0].outputs[0].text)\n",
    "# print(outputs[0].outputs[0].token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404\n",
      "Alright, so the user is asking, \"Where are all the aliens located?\" Hmm, that's a pretty bold and speculative question. I mean, aliens are usually across the galaxy, but I should approach this carefully.\n",
      "\n",
      "First, I need to consider the context. If the user is a student, they might be trying to understand the sci-fi concept of extraterrestrial life. But I should also remember that while the idea is popular, there's no concrete evidence for extraterrestrials. So, I shouldn't assume anything beyond what's widely accepted.\n",
      "\n",
      "I should provide a balanced answer. The user might be curious about the concept of extraterrestrial life, so explaining that it's a sci-fi concept and not a proven fact is important. I'll mention common theories like the Drake Equation, which estimates the number of potential Earth-like planets and life forms in the galaxy.\n",
      "\n",
      "Also, I should touch on the idea of biological contact, which some theories suggest might help us understand extraterrestrial life. But I should point out that this is speculative and not grounded in science.\n",
      "\n",
      "I should make sure to keep the language clear and informative, but also cautious about not giving away too much about the origins of life or other theories.\n",
      "\n",
      "So, I'll structure the answer by first acknowledging the speculative nature of the question, then explaining the concept of extraterrestrial life, and finally discussing theories related to biological contact without overstepping into areas where science doesn't provide clear answers.\n",
      "</think>\n",
      "\n",
      "The question about where all the aliens are located is a popular sci-fi concept, often explored through theories such as extraterrestrial life, the Drake Equation, or theories of biological contact. However, there is no concrete evidence to support the existence of extraterrestrial life or the idea of all aliens being located at specific locations. The concept is purely speculative and not based on any scientific evidence. If you're looking for more information on extraterrestrial life, I recommend exploring popular science fiction themes or theories like the Drake Equation.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "print(len(outputs[0].outputs[0].token_ids))\n",
    "print(outputs[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def format_check(completions):\n",
    "    pattern = r\"\\s*<think>.*?</think>\\s*<answer>.*?</answer>\\s*\"\n",
    "    matches = []\n",
    "    for content in completions:\n",
    "        start_tag = \"<think>\"\n",
    "        end_tag = \"</answer>\"\n",
    "\n",
    "        start_idx = content.find(start_tag)\n",
    "        end_idx = content.find(end_tag)\n",
    "\n",
    "        # make sure there is only one pair of <think> and <answer> tags\n",
    "        think_count = content.count(start_tag)\n",
    "        answer_count = content.count(end_tag)\n",
    "\n",
    "        if start_idx != -1 and end_idx != -1 and end_idx > start_idx and think_count == 1 and answer_count == 1:\n",
    "            content = content[start_idx:end_idx + len(end_tag)]\n",
    "\n",
    "        match = re.fullmatch(pattern, content, re.DOTALL)\n",
    "        matches.append(match)\n",
    "\n",
    "    return np.array([1.0 if m else 0.0 for m in matches])\n",
    "\n",
    "\n",
    "\n",
    "completion = '<think>\\nFirst, I need to simplify the expression \\\\(90r - 44r\\\\).\\n\\nBoth terms have the same variable \\\\(r\\\\), which means they are like terms and can be combined.\\n\\nI will subtract the coefficients: \\\\(90 - 44 = 46\\\\).\\n\\nTherefore, the simplified expression is \\\\(46r\\\\).\\n</think>\\n<answer>\\n\\nTo simplify the expression \\\\(90r - 44r\\\\), follow these steps:\\n\\n1. **Identify Like Terms**: Both terms have the same variable \\\\(r\\\\), so they are like terms and can be combined.\\n\\n2. **Subtract the Coefficients**:\\n   \\\\[\\n   90r - 44r = (90 - 44)r\\n   \\\\]\\n\\n3. **Calculate the Coefficient**:\\n   \\\\[\\n   90 - 44 = 46\\n   \\\\]\\n\\n4. **Write the Simplified Expression**:\\n   \\\\[\\n   46r\\n   \\\\]\\n\\n**Final Answer:**\\n\\\\[\\n\\\\boxed{46r}\\n\\\\]\\n</answer>'\n",
    "completions = [completion]\n",
    "format_scores = format_check(completions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xr1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
