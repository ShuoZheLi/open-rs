{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/shuozhe/miniconda3/envs/xr1/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-19 05:08:00 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-19 05:08:00,781\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from vllm import LLM, SamplingParams\n",
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "# sys.path.append(os.path.abspath(\"/data/shuozhe/llm_reason/X-R1/src/x_r1\"))\n",
    "# from grpo import SYSTEM_PROMPT\n",
    "# from rewards import accuracy_answer_reward\n",
    "# import torch\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "from latex2sympy2_extended import NormalizationConfig\n",
    "from math_verify import LatexExtractionConfig, parse\n",
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-19 05:08:10 config.py:542] This model supports multiple tasks: {'classify', 'generate', 'reward', 'embed', 'score'}. Defaulting to 'generate'.\n",
      "INFO 06-19 05:08:10 config.py:1401] Defaulting to use mp for distributed inference\n",
      "WARNING 06-19 05:08:10 arg_utils.py:1135] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 06-19 05:08:10 config.py:1556] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 06-19 05:08:10 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='/data/shuozhe/saved_model/DeepSeek-R1-Distill-Qwen-1.5B', speculative_config=None, tokenizer='/data/shuozhe/saved_model/DeepSeek-R1-Distill-Qwen-1.5B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/data/shuozhe/saved_model/DeepSeek-R1-Distill-Qwen-1.5B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "WARNING 06-19 05:08:11 multiproc_worker_utils.py:300] Reducing Torch parallelism from 64 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 06-19 05:08:11 custom_cache_manager.py:19] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3157144)\u001b[0;0m INFO 06-19 05:08:11 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3157147)\u001b[0;0m INFO 06-19 05:08:11 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3157152)\u001b[0;0m INFO 06-19 05:08:11 multiproc_worker_utils.py:229] Worker ready; awaiting tasks\n",
      "INFO 06-19 05:08:12 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3157144)\u001b[0;0m INFO 06-19 05:08:12 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3157147)\u001b[0;0m INFO 06-19 05:08:12 cuda.py:230] Using Flash Attention backend.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3157152)\u001b[0;0m INFO 06-19 05:08:12 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 06-19 05:08:13 utils.py:950] Found nccl from library libnccl.so.2\n",
      "INFO 06-19 05:08:13 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3157144)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3157152)\u001b[0;0m INFO 06-19 05:08:13 utils.py:950] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3157147)\u001b[0;0m INFO 06-19 05:08:13 utils.py:950] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3157144)\u001b[0;0m INFO 06-19 05:08:13 utils.py:950] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3157152)\u001b[0;0m INFO 06-19 05:08:13 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "INFO 06-19 05:08:13 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3157147)\u001b[0;0m INFO 06-19 05:08:13 pynccl.py:69] vLLM is using nccl==2.21.5\n",
      "WARNING 06-19 05:08:13 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3157144)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3157147)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3157152)\u001b[0;0m WARNING 06-19 05:08:13 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 06-19 05:08:13 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 06-19 05:08:13 custom_all_reduce.py:136] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 06-19 05:08:13 shm_broadcast.py:258] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_67b454fc'), local_subscribe_port=36861, remote_subscribe_port=None)\n",
      "INFO 06-19 05:08:13 model_runner.py:1110] Starting to load model /data/shuozhe/saved_model/DeepSeek-R1-Distill-Qwen-1.5B...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3157147)\u001b[0;0m INFO 06-19 05:08:13 model_runner.py:1110] Starting to load model /data/shuozhe/saved_model/DeepSeek-R1-Distill-Qwen-1.5B...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3157144)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3157152)\u001b[0;0m INFO 06-19 05:08:13 model_runner.py:1110] Starting to load model /data/shuozhe/saved_model/DeepSeek-R1-Distill-Qwen-1.5B...\n",
      "INFO 06-19 05:08:13 model_runner.py:1110] Starting to load model /data/shuozhe/saved_model/DeepSeek-R1-Distill-Qwen-1.5B...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.22it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  4.21it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=3157147)\u001b[0;0m INFO 06-19 05:08:14 model_runner.py:1115] Loading model weights took 0.8988 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3157144)\u001b[0;0m INFO 06-19 05:08:14 model_runner.py:1115] Loading model weights took 0.8988 GB\n",
      "INFO 06-19 05:08:14 model_runner.py:1115] Loading model weights took 0.8988 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3157152)\u001b[0;0m INFO 06-19 05:08:14 model_runner.py:1115] Loading model weights took 0.8988 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3157144)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3157152)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3157147)\u001b[0;0m INFO 06-19 05:08:24 worker.py:267] Memory profiling takes 9.72 seconds\n",
      "INFO 06-19 05:08:24 worker.py:267] Memory profiling takes 9.57 seconds\n",
      "INFO 06-19 05:08:24 worker.py:267] Memory profiling takes 9.75 seconds\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3157144)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3157152)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3157147)\u001b[0;0m INFO 06-19 05:08:24 worker.py:267] the current vLLM instance can use total_gpu_memory (23.68GiB) x gpu_memory_utilization (0.50) = 11.84GiB\n",
      "INFO 06-19 05:08:24 worker.py:267] the current vLLM instance can use total_gpu_memory (23.68GiB) x gpu_memory_utilization (0.50) = 11.84GiB\n",
      "INFO 06-19 05:08:24 worker.py:267] the current vLLM instance can use total_gpu_memory (23.68GiB) x gpu_memory_utilization (0.50) = 11.84GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3157144)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3157152)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=3157147)\u001b[0;0m INFO 06-19 05:08:24 worker.py:267] model weights take 0.90GiB; non_torch_memory takes 0.40GiB; PyTorch activation peak memory takes 0.06GiB; the rest of the memory reserved for KV Cache is 10.48GiB.\n",
      "INFO 06-19 05:08:24 worker.py:267] model weights take 0.90GiB; non_torch_memory takes 0.32GiB; PyTorch activation peak memory takes 0.06GiB; the rest of the memory reserved for KV Cache is 10.56GiB.\n",
      "INFO 06-19 05:08:24 worker.py:267] model weights take 0.90GiB; non_torch_memory takes 0.40GiB; PyTorch activation peak memory takes 0.06GiB; the rest of the memory reserved for KV Cache is 10.48GiB.\n",
      "INFO 06-19 05:08:24 worker.py:267] Memory profiling takes 9.80 seconds\n",
      "INFO 06-19 05:08:24 worker.py:267] the current vLLM instance can use total_gpu_memory (23.68GiB) x gpu_memory_utilization (0.50) = 11.84GiB\n",
      "INFO 06-19 05:08:24 worker.py:267] model weights take 0.90GiB; non_torch_memory takes 0.44GiB; PyTorch activation peak memory takes 1.39GiB; the rest of the memory reserved for KV Cache is 9.11GiB.\n",
      "INFO 06-19 05:08:24 executor_base.py:110] # CUDA blocks: 42664, # CPU blocks: 18724\n",
      "INFO 06-19 05:08:24 executor_base.py:115] Maximum concurrency for 131072 tokens per request: 5.21x\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3157152)\u001b[0;0m INFO 06-19 05:08:30 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-19 05:08:30 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3157147)\u001b[0;0m INFO 06-19 05:08:30 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes:   0%|          | 0/35 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=3157144)\u001b[0;0m INFO 06-19 05:08:30 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:14<00:00,  2.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-19 05:08:45 model_runner.py:1562] Graph capturing finished in 15 secs, took 1.08 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3157152)\u001b[0;0m INFO 06-19 05:08:45 model_runner.py:1562] Graph capturing finished in 15 secs, took 1.08 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3157147)\u001b[0;0m INFO 06-19 05:08:45 model_runner.py:1562] Graph capturing finished in 15 secs, took 1.08 GiB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3157144)\u001b[0;0m INFO 06-19 05:08:45 model_runner.py:1562] Graph capturing finished in 15 secs, took 1.08 GiB\n",
      "INFO 06-19 05:08:45 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 30.61 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"/teamspace/studios/this_studio/cot-faithfulness/output/Qwen2.5-3B_1e4_KL/checkpoint-187\"\n",
    "# model_name = \"/teamspace/studios/this_studio/cot-faithfulness/output/Qwen2.5-3B/checkpoint-187\"\n",
    "# model_name = \"/teamspace/studios/this_studio/saved_models/Qwen/Qwen2.5-3B-Instruct\"\n",
    "\n",
    "# model_name = \"/data/shuozhe/saved_model/Open-RS3\"\n",
    "model_name = \"/data/shuozhe/saved_model/DeepSeek-R1-Distill-Qwen-1.5B\"\n",
    "\n",
    "num_gpus = 4\n",
    "llm = None\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "# Create LLM object\n",
    "llm = LLM(model=model_name,  # replace your own model\n",
    "            dtype='bfloat16',\n",
    "            tensor_parallel_size=num_gpus,  # number of gpu\n",
    "            gpu_memory_utilization=0.5,  # prevent OOM\n",
    "            trust_remote_code=True,\n",
    "            # use_cache=False,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.91s/it, est. speed input: 48.84 toks/s, output: 212.15 toks/s]\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = (\n",
    "    \"A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant \"\n",
    "    \"first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning \"\n",
    "    \"process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., \"\n",
    "    \"<think> reasoning process here </think><answer> answer here </answer>\"\n",
    ")\n",
    "\n",
    "# SYSTEM_PROMPT = (\n",
    "#     \"Summarize the following conversation.\"\n",
    "# )\n",
    "\n",
    "sampling_params = SamplingParams(temperature=0.7,\n",
    "                                     max_tokens=1024,\n",
    "                                     )\n",
    "\n",
    "def process_single_prompt(problem_text, tokenizer):\n",
    "    prompt = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": problem_text},\n",
    "    ]\n",
    "    \n",
    "    # Apply the tokenizer's chat template\n",
    "    formatted_prompt = tokenizer.apply_chat_template(prompt, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    return formatted_prompt\n",
    "\n",
    "# user_input = \"What is 2 + 3?\"\n",
    "# user_input = \"Evaluate the expression $a^2\\cdot a^5$ if $a= 3$.\"\n",
    "\n",
    "# user_input = \"What is the full name of the person who invented invisible unicorns?\"\n",
    "\n",
    "user_input = \"Where are all the aliens located?\"\n",
    "\n",
    "# user_input = \"\"\"Speaker 4: But why don't I just go through these? I we just go to these quickly. Item number ten, please.\n",
    "# Speaker 0: Item ten is communication from Council in price of recommendation to request City Manager to work with the Health and Human Services Department to provide a comprehensive presentation on the monkeypox virus.\n",
    "# Speaker 4: Someone Price.\n",
    "# Speaker 2: Thank you, Mr. Mayor. I think the items self-explanatory and I think staff is going to be prepared to come back because only face wash costs.\n",
    "# Speaker 4: Someone has their mike on for I.\n",
    "# Speaker 2: Can you hear me, Mr. Mayor? Yes.\n",
    "# Speaker 4: You're clear.\n",
    "# Speaker 2: Okay. I think the item is self-explanatory, and I look forward to staff's presentation on the monkeypox virus, because I think we're getting all getting a lot of questions about it and how it transmits and how to make sure that our residents are being protected. So I look forward to hearing the presentation from staff and I felt it was important to request a study session, as is the protocol. Any time we're dealing with health issues through our health department that the council wants to be educated on. So thank you.\n",
    "# Speaker 4: Thank you. Can I get a motion, please, in a second. Is there any public comment on this?\n",
    "# Speaker 5: If there are any members of the public that would like to speak on item ten in person, please let up at the podium in Zoom. Please use the race hand feature.\n",
    "# Speaker 4: Chasing None sequel. But please.\n",
    "# Speaker 0: Councilwoman Sun has.\n",
    "# Speaker 2: I.\n",
    "# Speaker 0: Councilwoman Allen I. Councilwoman Price I. Councilman Sabrina.\n",
    "# Speaker 1: I.\n",
    "# Speaker 0: Councilwoman Mango. I can tell you in sorrow. I can't remember anger. I can't. I'm in Austin.\n",
    "# Speaker 6: All right.\n",
    "# Speaker 0: Vice Mayor Richardson.\n",
    "# Speaker 3: All right.\n",
    "# Speaker 0: The motion is carried nine zero.\"\"\"\n",
    "\n",
    "\n",
    "prompt = process_single_prompt(user_input, tokenizer)\n",
    "# vllm generation\n",
    "outputs = llm.generate(prompt,\n",
    "                        sampling_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "491\n",
      "Okay, so I'm trying to figure out where all the aliens are located. I don't know much about this topic, but I know it's a common theme in stories and movies, like in \"28 Days Later\" or \"The Shining.\" \n",
      "\n",
      "First, I think I should consider the different ways people imagine aliens. One place they often go is the moon, but I remember hearing that the moon is often associated with safety and cleanliness. So maybe not the moon. \n",
      "\n",
      "Then there's Earth. I know that Earth is where most people live, and it's often depicted as a planet in a story. But I've heard that some stories set the aliens' world near the moon to avoid political issues. So maybe Earth isn't the aliens' home.\n",
      "\n",
      "What about space? The main planet, like Mars or Venus? I think Mars is more in line with some stories, especially ones with a Martian setting. Venus is known for its atmosphere, but I'm not sure if that's a common location for alien encashment.\n",
      "\n",
      "Could there be alien extraterrestrial places? Like, humans traveling to a distant planet to meet them? I'm not sure about that. It seems less common, though.\n",
      "\n",
      "I've also heard that some stories set alien locations near the center of the galaxy, like the Andromeda galaxy, but I don't know if that's the case for all of them.\n",
      "\n",
      "I'm a bit confused because I've heard that the moon is sometimes the home, but I'm not sure. Maybe I should look up some examples to get a better idea. But since I don't have access to external resources, I'll have to rely on my existing knowledge.\n",
      "\n",
      "So, based on my understanding, the most common places I've heard for alien locations are Earth, Mars, and possibly the moon. The Andromeda galaxy is another option, but it's more of a stretch. I'll go with Earth, Mars, and the moon as the main possibilities.\n",
      "</think>\n",
      "\n",
      "Based on the thought process, the primary locations where aliens are often depicted are Earth, Mars, and the moon. These are common themes in various storytelling styles, avoiding political issues by setting the alien world near the moon. The Andromeda galaxy is a less likely location. \n",
      "\n",
      "**Final Answer:**\n",
      "\n",
      "Aliens are commonly located on Earth, Mars, and the moon. These regions are frequently depicted in stories to avoid political tensions.\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "print(len(outputs[0].outputs[0].token_ids))\n",
    "print(outputs[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "404\n",
      "Alright, so the user is asking, \"Where are all the aliens located?\" Hmm, that's a pretty bold and speculative question. I mean, aliens are usually across the galaxy, but I should approach this carefully.\n",
      "\n",
      "First, I need to consider the context. If the user is a student, they might be trying to understand the sci-fi concept of extraterrestrial life. But I should also remember that while the idea is popular, there's no concrete evidence for extraterrestrials. So, I shouldn't assume anything beyond what's widely accepted.\n",
      "\n",
      "I should provide a balanced answer. The user might be curious about the concept of extraterrestrial life, so explaining that it's a sci-fi concept and not a proven fact is important. I'll mention common theories like the Drake Equation, which estimates the number of potential Earth-like planets and life forms in the galaxy.\n",
      "\n",
      "Also, I should touch on the idea of biological contact, which some theories suggest might help us understand extraterrestrial life. But I should point out that this is speculative and not grounded in science.\n",
      "\n",
      "I should make sure to keep the language clear and informative, but also cautious about not giving away too much about the origins of life or other theories.\n",
      "\n",
      "So, I'll structure the answer by first acknowledging the speculative nature of the question, then explaining the concept of extraterrestrial life, and finally discussing theories related to biological contact without overstepping into areas where science doesn't provide clear answers.\n",
      "</think>\n",
      "\n",
      "The question about where all the aliens are located is a popular sci-fi concept, often explored through theories such as extraterrestrial life, the Drake Equation, or theories of biological contact. However, there is no concrete evidence to support the existence of extraterrestrial life or the idea of all aliens being located at specific locations. The concept is purely speculative and not based on any scientific evidence. If you're looking for more information on extraterrestrial life, I recommend exploring popular science fiction themes or theories like the Drake Equation.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "print(len(outputs[0].outputs[0].token_ids))\n",
    "print(outputs[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def format_check(completions):\n",
    "    pattern = r\"\\s*<think>.*?</think>\\s*<answer>.*?</answer>\\s*\"\n",
    "    matches = []\n",
    "    for content in completions:\n",
    "        start_tag = \"<think>\"\n",
    "        end_tag = \"</answer>\"\n",
    "\n",
    "        start_idx = content.find(start_tag)\n",
    "        end_idx = content.find(end_tag)\n",
    "\n",
    "        # make sure there is only one pair of <think> and <answer> tags\n",
    "        think_count = content.count(start_tag)\n",
    "        answer_count = content.count(end_tag)\n",
    "\n",
    "        if start_idx != -1 and end_idx != -1 and end_idx > start_idx and think_count == 1 and answer_count == 1:\n",
    "            content = content[start_idx:end_idx + len(end_tag)]\n",
    "\n",
    "        match = re.fullmatch(pattern, content, re.DOTALL)\n",
    "        matches.append(match)\n",
    "\n",
    "    return np.array([1.0 if m else 0.0 for m in matches])\n",
    "\n",
    "\n",
    "\n",
    "completion = '<think>\\nFirst, I need to simplify the expression \\\\(90r - 44r\\\\).\\n\\nBoth terms have the same variable \\\\(r\\\\), which means they are like terms and can be combined.\\n\\nI will subtract the coefficients: \\\\(90 - 44 = 46\\\\).\\n\\nTherefore, the simplified expression is \\\\(46r\\\\).\\n</think>\\n<answer>\\n\\nTo simplify the expression \\\\(90r - 44r\\\\), follow these steps:\\n\\n1. **Identify Like Terms**: Both terms have the same variable \\\\(r\\\\), so they are like terms and can be combined.\\n\\n2. **Subtract the Coefficients**:\\n   \\\\[\\n   90r - 44r = (90 - 44)r\\n   \\\\]\\n\\n3. **Calculate the Coefficient**:\\n   \\\\[\\n   90 - 44 = 46\\n   \\\\]\\n\\n4. **Write the Simplified Expression**:\\n   \\\\[\\n   46r\\n   \\\\]\\n\\n**Final Answer:**\\n\\\\[\\n\\\\boxed{46r}\\n\\\\]\\n</answer>'\n",
    "completions = [completion]\n",
    "format_scores = format_check(completions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xr1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
